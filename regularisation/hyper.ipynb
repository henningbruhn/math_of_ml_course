{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of regularisation strength\n",
    "\n",
    "The strength of the regularisation is a parameter, often termed a *hyperparameter*, of the ML algorithm that needs to be fine-tuned. Often there are a number of these. Others are the learning rate, the number of hidden neurons or even the whole architecture of the neural networks. Hyperparameter tuning aims to find the set of hyperparameters that yield the best, the most efficient algorithm. Special care needs to be taken that hyperparameter tuning does not exploit the test set. In practice this means: as usual, the test set should only be fed into the algorithm (or algorithms) once, at the end.\n",
    "\n",
    "Because I am lazy we use the fashion data set again, and also the neural network class from scikit-learn. (Tensorflow would be better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import zero_one_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data from openml.org\n",
    "# see https://www.openml.org/d/40996\n",
    "fashion = fetch_openml('Fashion-MNIST', cache=True)\n",
    "fashion.target = fashion.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "X, y = fashion[\"data\"]/255, fashion[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: Han Xiao, Kashif Rasul, Roland Vollgraf  \n",
      "**Source**: [Zalando Research](https://github.com/zalandoresearch/fashion-mnist)  \n",
      "**Please cite**: Han Xiao and Kashif Rasul and Roland Vollgraf, Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms, arXiv, cs.LG/1708.07747  \n",
      "\n",
      "Fashion-MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. \n",
      "\n",
      "Raw data available at: https://github.com/zalandoresearch/fashion-mnist\n",
      "\n",
      "### Target classes\n",
      "Each training and test example is assigned to one of the following labels:\n",
      "Label  Description  \n",
      "0  T-shirt/top  \n",
      "1  Trouser  \n",
      "2  Pullover  \n",
      "3  Dress  \n",
      "4  Coat  \n",
      "5  Sandal  \n",
      "6  Shirt  \n",
      "7  Sneaker  \n",
      "8  Bag  \n",
      "9  Ankle boot\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "print(fashion.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training, test set and validation set\n",
    "\n",
    "We split the data set into **three** parts: training set, validation set and test set. The validation set is used for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pick a large random subset as training set\n",
    "# the rest makes up the test set\n",
    "train_size=5000\n",
    "val_size=5000\n",
    "X, y = sklearn.utils.shuffle(X,y)\n",
    "\n",
    "# introduce validation set\n",
    "# here, it is a bit large in comparison with the training set\n",
    "X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter tuning\n",
    "\n",
    "We search semi-systematically for the best L2-penalty alpha. The neural network classifier <code>MLPClassifier</code> in scikit-learn has a parameter <code>alpha</code> with which large weights can be penalised. More precisely, the loss function comprises a regularisation term\n",
    "\n",
    "$$\n",
    "R(w)=\\alpha||w||_2^2,\n",
    "$$\n",
    "\n",
    "where $w$ is the weight vector of the neural network.\n",
    "\n",
    "Note that the network here is over-parameterised: the training set comprises just 5000 samples, while the network has about 90000 weights. And indeed, we'll be able to achieve zero or close to zero training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha / iterations: 0 / 167\n",
      "train / val error: 0.2% / 15.9%\n",
      " \n",
      "alpha / iterations: 0.001 / 255\n",
      "train / val error: 0.0% / 15.8%\n",
      " \n",
      "alpha / iterations: 0.01 / 251\n",
      "train / val error: 0.0% / 15.8%\n",
      " \n",
      "alpha / iterations: 0.1 / 553\n",
      "train / val error: 0.0% / 15.6%\n",
      " \n",
      "alpha / iterations: 1 / 180\n",
      "train / val error: 3.9% / 14.9%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes=(100,100)\n",
    "alphas=[0,0.001,0.01,0.1,1]  \n",
    "for alpha in alphas:\n",
    "    net=MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,max_iter=3000,\\\n",
    "                      alpha=alpha,solver='sgd',learning_rate='constant',learning_rate_init=0.01)\n",
    "    net.fit(X_train,y_train)\n",
    "    train_err=zero_one_loss(y_train,net.predict(X_train))\n",
    "    val_err=zero_one_loss(y_val,net.predict(X_val))\n",
    "    test_err=zero_one_loss(y_test,net.predict(X_test))\n",
    "    print(\"alpha / iterations: {} / {}\".format(alpha,net.n_iter_))\n",
    "    print(\"train / val error: {:.1f}% / {:.1f}%\".format(train_err*100,val_err*100))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha / iterations: 0.5 / 291\n",
      "train / val error: 0.9% / 15.2%\n",
      " \n",
      "alpha / iterations: 1 / 216\n",
      "train / val error: 3.1% / 14.5%\n",
      " \n",
      "alpha / iterations: 2 / 165\n",
      "train / val error: 9.7% / 16.7%\n",
      " \n",
      "alpha / iterations: 10 / 131\n",
      "train / val error: 21.6% / 23.3%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes=(100,100)\n",
    "alphas=[0.5,1,2,10]  \n",
    "for alpha in alphas:\n",
    "    net=MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,max_iter=3000,\\\n",
    "                      alpha=alpha,solver='sgd',learning_rate='constant',learning_rate_init=0.01)\n",
    "    net.fit(X_train,y_train)\n",
    "    train_err=zero_one_loss(y_train,net.predict(X_train))\n",
    "    val_err=zero_one_loss(y_val,net.predict(X_val))\n",
    "    print(\"alpha / iterations: {} / {}\".format(alpha,net.n_iter_))\n",
    "    print(\"train / val error: {:.1f}% / {:.1f}%\".format(train_err*100,val_err*100))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha / iterations: 0.8 / 310\n",
      "train / val error: 1.7% / 14.7%\n",
      " \n",
      "alpha / iterations: 0.9 / 190\n",
      "train / val error: 4.9% / 16.0%\n",
      " \n",
      "alpha / iterations: 1.1 / 211\n",
      "train / val error: 6.7% / 15.8%\n",
      " \n",
      "alpha / iterations: 1.2 / 220\n",
      "train / val error: 5.0% / 15.5%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes=(100,100)\n",
    "alphas=[0.8,0.9,1.1,1.2]  \n",
    "for alpha in alphas:\n",
    "    net=MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,max_iter=3000,\\\n",
    "                      alpha=alpha,solver='sgd',learning_rate='constant',learning_rate_init=0.01)\n",
    "    net.fit(X_train,y_train)\n",
    "    train_err=zero_one_loss(y_train,net.predict(X_train))\n",
    "    val_err=zero_one_loss(y_val,net.predict(X_val))\n",
    "    print(\"alpha / iterations: {} / {}\".format(alpha,net.n_iter_))\n",
    "    print(\"train / val error: {:.1f}% / {:.1f}%\".format(train_err*100,val_err*100))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, alpha=1 seems to give the best results. Note that we look here for the error on the validation set -- it works as a sort of \"test set\" for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training and test run\n",
    "Train on all of available data, ie training set plus validation set. Why? Because more data is always better and now that the validation set has been used for fine-tuning we can simply lump it with the training set.\n",
    "Then compute test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha / iterations: 1 / 180\n",
      "train / test error: 6.9% / 14.0%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "alpha=1\n",
    "net=MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,max_iter=3000,\\\n",
    "                  alpha=alpha,solver='sgd',learning_rate='constant',learning_rate_init=0.01)\n",
    "net.fit(X[:train_size+val_size],y[:train_size+val_size])\n",
    "train_err=zero_one_loss(y[:train_size+val_size],net.predict(X[:train_size+val_size]))\n",
    "test_err=zero_one_loss(y[train_size+val_size:],net.predict(X[train_size+val_size:]))\n",
    "print(\"alpha / iterations: {} / {}\".format(alpha,net.n_iter_))\n",
    "print(\"train / test error: {:.1f}% / {:.1f}%\".format(train_err*100,test_err*100))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the sake of comparison, let's train an unregularised network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha / iterations: 0 / 170\n",
      "train / test error: 0.2% / 15.0%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "alpha=0\n",
    "net=MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,max_iter=3000,\\\n",
    "                  alpha=alpha,solver='sgd',learning_rate='constant',learning_rate_init=0.01)\n",
    "net.fit(X[:train_size+val_size],y[:train_size+val_size])\n",
    "train_err=zero_one_loss(y[:train_size+val_size],net.predict(X[:train_size+val_size]))\n",
    "test_err=zero_one_loss(y[train_size+val_size:],net.predict(X[train_size+val_size:]))\n",
    "print(\"alpha / iterations: {} / {}\".format(alpha,net.n_iter_))\n",
    "print(\"train / test error: {:.1f}% / {:.1f}%\".format(train_err*100,test_err*100))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we'd need to repeat this experiment to make sure that the results are not a statistical fluke. \n",
    "\n",
    "Moreover, what we did here was a at most semi-systematic search for one best hyperparameter. There are more sophisticated methods. A good starting point is the [tutorial](https://scikit-learn.org/stable/modules/grid_search.html) by scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
