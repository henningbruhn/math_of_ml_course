{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implicit regularisation through SGD\n",
    "\n",
    "Why do neural networks rarely overfit? One possible explanation is that SGD _implicitly_ leads to regularisation. That is, among all minima of the loss function SGD picks one with small weight norm. And we know that regularisation inhibits overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data from openml.org\n",
    "# see https://www.openml.org/d/40996\n",
    "fashion = fetch_openml('Fashion-MNIST', cache=True)\n",
    "fashion.target = fashion.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "X, y = fashion[\"data\"], fashion[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images of data set have values in 0,...,255\n",
    "# first scale all images to have values in [0,1] \n",
    "X=X/255\n",
    "# we permute the data randomly\n",
    "X, y = sklearn.utils.shuffle(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad local minima\n",
    "\n",
    "We perform three experiments, where we train a shallow neural network on the fashion data set, each time with different training sets. The training sets are made up as follows:\n",
    "\n",
    "1. normal data set of 5000 images\n",
    "2. the normal data set of 5000 images *plus* 5000 images with random labels\n",
    "3. the normal data set of 5000 images *plus* 5000 other images with proper labels \n",
    "\n",
    "Each setting is repeated 10 times to gain a bit of statistical stability. Because it's simple I use here the neural network class of scikit-learn, MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden=100\n",
    "\n",
    "def analyse(X,y,train_size,corrupted_y,net):\n",
    "    X_train,X_test=X[:train_size:],X[train_size:]\n",
    "    y_train,y_test=y[:train_size],y[train_size:]\n",
    "    corr_size=len(corrupted_y)\n",
    "    corr_X_train=X[:corr_size]\n",
    "    result={}\n",
    "    result[\"iterations\"]=net.n_iter_\n",
    "    result[\"train err\"]=zero_one_loss(y_train,net.predict(X_train))\n",
    "    result[\"test err\"]=zero_one_loss(y_test,net.predict(X_test))\n",
    "    result[\"corr train err\"]=zero_one_loss(corrupted_y,net.predict(corr_X_train))\n",
    "    result[\"L2-norm\"]=math.sqrt(np.linalg.norm(net.coefs_[0])**2+np.linalg.norm(net.coefs_[1])**2)\n",
    "    result[\"L1-norm\"]=np.linalg.norm(net.coefs_[0].flat,1)+np.linalg.norm(net.coefs_[1].flat,1)\n",
    "    result[\"L2-norm bias\"]=math.sqrt(np.linalg.norm(net.intercepts_[0])**2+np.linalg.norm(net.intercepts_[1])**2)\n",
    "    result[\"L1-norm bias\"]=np.linalg.norm(net.intercepts_[0].flat,1)+np.linalg.norm(net.intercepts_[1].flat,1)\n",
    "    return result\n",
    "    \n",
    "def perform_passes(setup,X,y,train_size,corrupted_y,num_passes):\n",
    "    res_list=[]\n",
    "    corr_size=len(corrupted_y)\n",
    "    corr_X_train=X[:corr_size]\n",
    "    net=MLPClassifier(hidden_layer_sizes=hidden,max_iter=3000,alpha=0,\\\n",
    "                  solver='sgd',learning_rate='constant',learning_rate_init=0.01)\n",
    "    for p in range(num_passes):\n",
    "        net.fit(corr_X_train,corrupted_y)\n",
    "        res=analyse(X,y,train_size,corrupted_y,net)\n",
    "        res[\"setup\"]=setup\n",
    "        res_list.append(res)\n",
    "    return res_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We force SGD into other local minima by adding more data to the training set. For comparison we also compute a run of SGD without adding any data. The added data will have corrupt labels to force local minima that generalise badly. To achieve some statistical stability all tests are run five times. (More would be better but I didn't like to wait that long.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>train err</th>\n",
       "      <th>test err</th>\n",
       "      <th>corr train err</th>\n",
       "      <th>L2-norm</th>\n",
       "      <th>L1-norm</th>\n",
       "      <th>L2-norm bias</th>\n",
       "      <th>L1-norm bias</th>\n",
       "      <th>setup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163338</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.575302</td>\n",
       "      <td>4880.257044</td>\n",
       "      <td>2.066592</td>\n",
       "      <td>16.758682</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.193620</td>\n",
       "      <td>4837.514777</td>\n",
       "      <td>2.058899</td>\n",
       "      <td>16.626979</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163708</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.199532</td>\n",
       "      <td>4862.033601</td>\n",
       "      <td>2.125582</td>\n",
       "      <td>17.056574</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164677</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.229235</td>\n",
       "      <td>4846.440341</td>\n",
       "      <td>1.998544</td>\n",
       "      <td>15.824510</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163492</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.126797</td>\n",
       "      <td>4825.836854</td>\n",
       "      <td>1.969107</td>\n",
       "      <td>15.860322</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163908</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.372054</td>\n",
       "      <td>4841.849561</td>\n",
       "      <td>2.141500</td>\n",
       "      <td>17.089733</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163277</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.616897</td>\n",
       "      <td>4910.291207</td>\n",
       "      <td>2.131746</td>\n",
       "      <td>16.320833</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.704331</td>\n",
       "      <td>4900.182724</td>\n",
       "      <td>2.176662</td>\n",
       "      <td>16.873327</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.202956</td>\n",
       "      <td>4820.626332</td>\n",
       "      <td>2.056383</td>\n",
       "      <td>15.854893</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.180926</td>\n",
       "      <td>4840.389921</td>\n",
       "      <td>2.056107</td>\n",
       "      <td>16.146716</td>\n",
       "      <td>no corruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>65.433392</td>\n",
       "      <td>11636.990293</td>\n",
       "      <td>3.205018</td>\n",
       "      <td>25.831823</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543923</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>65.384010</td>\n",
       "      <td>11814.315522</td>\n",
       "      <td>3.381415</td>\n",
       "      <td>29.169831</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>64.675741</td>\n",
       "      <td>11580.666301</td>\n",
       "      <td>3.494704</td>\n",
       "      <td>29.373468</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540938</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>63.697563</td>\n",
       "      <td>11483.410222</td>\n",
       "      <td>3.485002</td>\n",
       "      <td>28.911996</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548877</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>65.481112</td>\n",
       "      <td>11734.505861</td>\n",
       "      <td>3.519499</td>\n",
       "      <td>29.681130</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549769</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>63.759183</td>\n",
       "      <td>11426.711237</td>\n",
       "      <td>3.252600</td>\n",
       "      <td>27.013648</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548215</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>65.373045</td>\n",
       "      <td>11667.250359</td>\n",
       "      <td>3.517663</td>\n",
       "      <td>28.821890</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544385</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>65.324666</td>\n",
       "      <td>11653.414497</td>\n",
       "      <td>3.360565</td>\n",
       "      <td>28.302821</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551415</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>66.077206</td>\n",
       "      <td>11857.391053</td>\n",
       "      <td>3.599812</td>\n",
       "      <td>30.029038</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549092</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64.268655</td>\n",
       "      <td>11619.442218</td>\n",
       "      <td>3.591717</td>\n",
       "      <td>29.972505</td>\n",
       "      <td>random labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.222130</td>\n",
       "      <td>5575.901512</td>\n",
       "      <td>2.299459</td>\n",
       "      <td>18.976100</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145767</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.356980</td>\n",
       "      <td>5624.328356</td>\n",
       "      <td>2.374882</td>\n",
       "      <td>19.166050</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145983</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.486185</td>\n",
       "      <td>5620.679714</td>\n",
       "      <td>2.384838</td>\n",
       "      <td>19.477854</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145883</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.965837</td>\n",
       "      <td>5550.696220</td>\n",
       "      <td>2.435334</td>\n",
       "      <td>18.662726</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.765908</td>\n",
       "      <td>5530.828388</td>\n",
       "      <td>2.454107</td>\n",
       "      <td>19.519510</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147583</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.729345</td>\n",
       "      <td>5561.308669</td>\n",
       "      <td>2.344177</td>\n",
       "      <td>18.175728</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.598945</td>\n",
       "      <td>5662.974430</td>\n",
       "      <td>2.478263</td>\n",
       "      <td>20.042147</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146433</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.332350</td>\n",
       "      <td>5609.840991</td>\n",
       "      <td>2.325693</td>\n",
       "      <td>18.775340</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.011016</td>\n",
       "      <td>5577.291248</td>\n",
       "      <td>2.348531</td>\n",
       "      <td>19.162788</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>29.905524</td>\n",
       "      <td>5572.904325</td>\n",
       "      <td>2.269953</td>\n",
       "      <td>18.454734</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iterations  train err  test err  corr train err    L2-norm       L1-norm  \\\n",
       "0          575        0.0  0.163338          0.0000  25.575302   4880.257044   \n",
       "1          523        0.0  0.163400          0.0000  25.193620   4837.514777   \n",
       "2          531        0.0  0.163708          0.0000  25.199532   4862.033601   \n",
       "3          522        0.0  0.164677          0.0000  25.229235   4846.440341   \n",
       "4          515        0.0  0.163492          0.0000  25.126797   4825.836854   \n",
       "5          550        0.0  0.163908          0.0000  25.372054   4841.849561   \n",
       "6          585        0.0  0.163277          0.0000  25.616897   4910.291207   \n",
       "7          592        0.0  0.163308          0.0000  25.704331   4900.182724   \n",
       "8          538        0.0  0.163662          0.0000  25.202956   4820.626332   \n",
       "9          517        0.0  0.163662          0.0000  25.180926   4840.389921   \n",
       "10         974        0.0  0.546600          0.0000  65.433392  11636.990293   \n",
       "11        1014        0.0  0.543923          0.0000  65.384010  11814.315522   \n",
       "12         946        0.0  0.547000          0.0000  64.675741  11580.666301   \n",
       "13         870        0.0  0.540938          0.0000  63.697563  11483.410222   \n",
       "14        1011        0.0  0.548877          0.0000  65.481112  11734.505861   \n",
       "15         862        0.0  0.549769          0.0001  63.759183  11426.711237   \n",
       "16         981        0.0  0.548215          0.0000  65.373045  11667.250359   \n",
       "17         980        0.0  0.544385          0.0000  65.324666  11653.414497   \n",
       "18         986        0.0  0.551415          0.0000  66.077206  11857.391053   \n",
       "19         920        0.0  0.549092          0.0001  64.268655  11619.442218   \n",
       "20         504        0.0  0.149083          0.0000  30.222130   5575.901512   \n",
       "21         532        0.0  0.145767          0.0000  30.356980   5624.328356   \n",
       "22         544        0.0  0.145983          0.0000  30.486185   5620.679714   \n",
       "23         484        0.0  0.145883          0.0000  29.965837   5550.696220   \n",
       "24         481        0.0  0.144083          0.0000  29.765908   5530.828388   \n",
       "25         473        0.0  0.147583          0.0000  29.729345   5561.308669   \n",
       "26         558        0.0  0.145700          0.0000  30.598945   5662.974430   \n",
       "27         518        0.0  0.146433          0.0000  30.332350   5609.840991   \n",
       "28         500        0.0  0.145900          0.0000  30.011016   5577.291248   \n",
       "29         483        0.0  0.146250          0.0000  29.905524   5572.904325   \n",
       "\n",
       "    L2-norm bias  L1-norm bias          setup  \n",
       "0       2.066592     16.758682  no corruption  \n",
       "1       2.058899     16.626979  no corruption  \n",
       "2       2.125582     17.056574  no corruption  \n",
       "3       1.998544     15.824510  no corruption  \n",
       "4       1.969107     15.860322  no corruption  \n",
       "5       2.141500     17.089733  no corruption  \n",
       "6       2.131746     16.320833  no corruption  \n",
       "7       2.176662     16.873327  no corruption  \n",
       "8       2.056383     15.854893  no corruption  \n",
       "9       2.056107     16.146716  no corruption  \n",
       "10      3.205018     25.831823  random labels  \n",
       "11      3.381415     29.169831  random labels  \n",
       "12      3.494704     29.373468  random labels  \n",
       "13      3.485002     28.911996  random labels  \n",
       "14      3.519499     29.681130  random labels  \n",
       "15      3.252600     27.013648  random labels  \n",
       "16      3.517663     28.821890  random labels  \n",
       "17      3.360565     28.302821  random labels  \n",
       "18      3.599812     30.029038  random labels  \n",
       "19      3.591717     29.972505  random labels  \n",
       "20      2.299459     18.976100          large  \n",
       "21      2.374882     19.166050          large  \n",
       "22      2.384838     19.477854          large  \n",
       "23      2.435334     18.662726          large  \n",
       "24      2.454107     19.519510          large  \n",
       "25      2.344177     18.175728          large  \n",
       "26      2.478263     20.042147          large  \n",
       "27      2.325693     18.775340          large  \n",
       "28      2.348531     19.162788          large  \n",
       "29      2.269953     18.454734          large  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size=5000\n",
    "corruption_size=5000\n",
    "num_passes=10 # how often we repeat each experiment to get statistically stable results\n",
    "\n",
    "results=[]\n",
    "\n",
    "## vanilla training, no corruption\n",
    "results.extend(perform_passes(\"no corruption\",X,y,train_size,y[:train_size],num_passes))\n",
    "\n",
    "## training set + set with random labels\n",
    "yy=np.array(y[:corruption_size])\n",
    "np.random.shuffle(yy)\n",
    "corrupted_y=np.hstack((y[:train_size],yy))\n",
    "results.extend(perform_passes(\"random labels\",X,y,train_size,corrupted_y,num_passes))\n",
    "\n",
    "## train with real labels but training set of size train_size+corruption_size\n",
    "large_size=train_size+corruption_size\n",
    "results.extend(perform_passes(\"large\",X,y,large_size,y[:large_size],num_passes))\n",
    "\n",
    "F=pd.DataFrame(results)\n",
    "F.to_csv(\"corruption.txt\")\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>train err</th>\n",
       "      <th>test err</th>\n",
       "      <th>corr train err</th>\n",
       "      <th>L2-norm</th>\n",
       "      <th>L1-norm</th>\n",
       "      <th>L2-norm bias</th>\n",
       "      <th>L1-norm bias</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setup</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no corruption</th>\n",
       "      <td>544.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163643</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25.340165</td>\n",
       "      <td>4856.542236</td>\n",
       "      <td>2.078112</td>\n",
       "      <td>16.441257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>507.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146267</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.137422</td>\n",
       "      <td>5588.675385</td>\n",
       "      <td>2.371524</td>\n",
       "      <td>19.041298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random labels</th>\n",
       "      <td>954.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>64.947457</td>\n",
       "      <td>11647.409756</td>\n",
       "      <td>3.440800</td>\n",
       "      <td>28.710815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               iterations  train err  test err  corr train err    L2-norm  \\\n",
       "setup                                                                       \n",
       "no corruption       544.8        0.0  0.163643         0.00000  25.340165   \n",
       "large               507.7        0.0  0.146267         0.00000  30.137422   \n",
       "random labels       954.4        0.0  0.547022         0.00002  64.947457   \n",
       "\n",
       "                    L1-norm  L2-norm bias  L1-norm bias  \n",
       "setup                                                    \n",
       "no corruption   4856.542236      2.078112     16.441257  \n",
       "large           5588.675385      2.371524     19.041298  \n",
       "random labels  11647.409756      3.440800     28.710815  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by setup and take the mean\n",
    "means=F.groupby(\"setup\").mean()\n",
    "means=means.sort_values(by=[\"L2-norm\"])\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected adding corrupted data to the training set leads to local minima that generalise badly. Indeed, below the test error for the unadulterated is low (\"no corruption\", test error about 16.6%), while all aldulterated training sets lead to test errors above 50%. Note that in all these cases the error on the original training set is 0% or close to 0%. For comparision, the test error on a training set on 10000 samples is shown -- the same size as the aldulterated training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAACICAYAAADqIJGqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAg2ElEQVR4nO3deVhUdf//8SfD4kKIiqClVmqJuyKVmRo6iizFbWKLmVpkWaJepragYYsWqGXm2qKlhZV3Iqb3jaL4LbXFSLRSbM9SSCUVWQSSgPP7g59zS4grMMfh9bgur4sz53zOvGaG9/jmc87McTIMw0BERETEgVjsHUBERESkqqnBEREREYejBkdEREQcjhocERERcThqcERERMThuNg7QG1RXFzC8eMF9o5RqUaN6ps2n5mzgfJdKnvl8/b2uOAxZqljs7ymZshhhgxmyWGGDPbIUVktawanhri4ONs7wlmZOZ+Zs4HyXSqz5zudWbIqh7kygDlymCEDmCeHGhwRERFxOGpwRERExOHoHJwaEjZ5rb0jiDi8t6Os1bp/1bFIzaiKWtYMjoiIiDgcNTgiIiLicNTgiIiIiMNRgyMiIiIORw2OiIiIOBw1OCIiIuJw1OCIiIiIw7FLg/P000+zYMGCatm3r68vhw8fPus2UVFRLF68+IL2m5CQwAMPPHAJyURERKSmaAZHREREHM5ZG5yMjAx69+5NTEwMw4cPB+D//u//CAsLIygoiPDwcL7//nsAUlJSuOeee5gzZw4hISFYrVa++uorAI4fP86DDz6I1Wpl9OjR5OXl2e7jhx9+YOjQoQQHBzNo0CA+/fTTcvuLiYmhf//+hIeH8+233zJixAh69erF/Pnzz/ngFi1aRFBQEAMGDOCRRx4hNzfXti4zM5Phw4fTr18/xo4dS0FB2ZVPf/nlF4YPH05QUBBhYWHs2bOnwn6/+uorBg8eTGhoKCEhIWzYsOGcWURERKTmnHMGJzs7m/bt27NixQqKi4uJiopixowZbNy4EavVyqxZs2zbfvfdd3Tt2pUNGzYwbNgwXnvtNQCWLFlCo0aN+Pjjj3nmmWf47LPPACgtLWXSpEkMHz6cpKQkXnjhBSZPnsyJEycA2Lt3L/3792fz5s1YLBamT5/Om2++ybJly3jjjTc4efJkpbnT0tJ47733WL16NZs2baKoqIgVK1bY1n/66afMnz+fzZs3k5OTw6pVqygtLWXixIkMGjSIjRs38txzzxEZGUlxcXG5fc+aNYspU6awfv16XnvtNTZv3nwBT7mIiIhUt3Nei+rvv/8mMDCwbGMXF7744gtcXV0BuOGGG1izZo1tW3d3dwYMGABAx44dWbVqFQCpqamMHj0agBYtWnDTTTcBZTNER48e5bbbbgOgc+fOXHXVVezZsweLxUKDBg3o0aMHANdffz0NGzakXr16XH/99ZSUlJCVlcWVV155xtydOnViy5YtuLm5AeDn50d6erpt/a233krjxo0BCAwM5JtvvqFXr14cOHCAIUOGAODv70/jxo35+uuvy+3by8uLjz76CC8vL9q0acOcOXPO9TSKSA3w9vawdwQRqQJVUcvnbHCcnZ254oorbMtxcXGsWbOGoqIiioqKcHJysq3z8PhfIIvFQmlpKQA5OTnl1jVo0ACArKwsPDw8yu2jQYMGZGVl0aRJE9zd3cvtr379+gA4OTlhsVgoKSmpNHdhYSGxsbGkpKTYMvTt29e2/lRzcyp3bm4uubm5lJSUEBoaalt34sQJsrOzy+07JiaG1157jYiICOrWrcukSZMIDg6uNIuI1IwjR/LOvdH/p2ZIxLyqopYv6Griu3btYsmSJaxatYoWLVrw+eefM23atHOOa9CgQbnzbrKysmjZsiVeXl7k5ORgGIatycnOzsbLy+tCYp3RO++8w++//05CQgLu7u7MnTuXzMxM2/qcnBzbz7m5uXh6euLj44O7uztJSUkV9peQkGD7uUmTJkybNo1p06bx2WefMX78ePr06VOuIRMRERH7uaBPUWVlZeHl5cWVV15JYWEhCQkJFBQU2GZqKtOtWzfbeSoHDhxg586dQNnhqmbNmrF+/XqgrIE6evQoXbp0uZjHUs6xY8do1aoV7u7u/PHHH2zZsoX8/Hzb+m3btpGTk0NJSQnJycn4+/vTvHlzmjVrZmtwsrKymDRpku0EZCg7ZDdixAj+/PNPoOxQnIuLC87OzpecWURERKrGBc3g9OnTh/fff5+AgABatmzJ1KlT2b17N2PHjj3rd8Q88sgjTJw4EavVSps2bRg4cCBQdqjplVde4dlnn2XhwoXUq1ePefPm2Q5FXYqhQ4cyfvx4rFYrnTp1YsqUKYwdO5Zly5YB0K9fP8aPH09GRgadOnViyJAhtjzPPfccr776KhaLhYiIiHJ5XF1dufPOO22P12KxEB0dTd26dS85s4iIiFQNJ8MwDHuHqA3CJq+1dwQRh/d2lPW8t72Yc3BUxyI1oypqWV/0JyIiIg5HDY6IiIg4HDU4IiIi4nDU4IiIiIjDUYMjIiIiDkefoqpBF/LNjDXN29vDtPnMnA2U71LZK9/FfpOxGZ5Ls7ymZshhhgxmyWGGDPbIoU9RiYiISK2hBkdEREQcjhocERERcThqcERERMThqMERERERh3NBF9uUi1dbrmFzIdcPEbnc1JY6vtzofUfORDM4IiIi4nDU4IiIiIjDUYMjIiIiDkcNjoiIiDgcNTgiIiLicNTgiIiIiMOpFQ1OSkoKgYGB9o4hIiIiNaRWNDgiIiJSu9SqL/orLCxkypQpfP/99/z9998EBQXx1FNPATBixAi6d+/Opk2bePHFF/Hx8eHxxx/n2LFj9OzZk8zMTIKCgggPD2fnzp3ExMSQm5tLo0aNmDNnDi1btrTzoxMREZFTatUMzgcffEB+fj5JSUmsWbOGhIQEUlNTbevT0tJITEyke/fuzJ49G39/f5KTk+nTpw9ffPEFAPn5+UyYMIFJkyaRnJzMyJEjmTBhgr0ekoiIiJxBrZrBefDBBxkxYgROTk54enpy/fXXk5GRwQ033ABAQEAAFktZz5eamsqYMWMACAwMxMfHx3a7u7s7vXr1AuD222/nueee4+DBg1x11VV2eFTm4u3tcVntt6oo36Uxez4xN7P8/pghhxkygDly1KoG5/fff2fmzJns27cPi8XC4cOHCQ8Pt6339PS0/Zybm0uDBg1sy02bNrXdnpmZSXBwsG2dm5sbWVlZanCAI0fyqnyf3t4e1bLfqqJ8l8Ze+czwBixVwwy/32aoMzNksEeOymq5VjU406dPp2PHjixatAhnZ2eGDh1a6bbu7u6cOHHCtnzkyBEAfHx8aN26NQkJCdWeV0RERC5OrToH59ixY7Rv3x5nZ2c+//xz9u/fT35+/hm37dKlC5s2bQLgk08+4c8//wSga9euHDlyhG+//RaA9PR0nnjiCQzDqJkHISIiIudUq2ZwxowZwwsvvMDChQsJDAxk3LhxvPLKK3To0KHCtk888QSTJ08mMTGRW2+9lW7duuHk5ETdunWZP38+M2bMID8/H1dXVyZMmICTk5MdHpGIiIicSa1ocHr06EFycjJAuXNnAO677z4A4uLiyt3etm1b1q1bZ2tchgwZgodH2XE+Pz8/4uPjqzu2iIiIXKRadYjqQsyaNYvnn38egF9//ZV9+/bRqVMnO6cSERGR81ErZnAuRkREBE8++SSBgYFYLBaeeeYZmjVrZu9YIiIich7U4FTCx8eH5cuX2zuGiIiIXAQdohIRERGHowZHREREHI4OUdWQ/8wZZIpvmKyMWb4BU8TMzFLHZqlXM+QwQwYxJ83giIiIiMNRgyMiIiIORw2OiIiIOBw1OCIiIuJw1OCIiIiIw9GnqGpI2OS19o5gd29HWe0dQeSSqI6rh94bpDpoBkdEREQcjhocERERcThqcERERMThqMERERERh6MGR0RERByOGhwRERFxOGpwRERExOHU+gbnww8/tP0cHBzM0aNH7ZhGREREqkKtbnCOHDnC0qVLbctJSUk0adLEjolERESkKlxwg5ORkUHv3r159913CQsLo0+fPqxfvx6A0tJS5s6dS3BwMMHBwURFRVFQUFBhH4ZhEBsbi9VqJSgoyNZknG38iBEjmDt3LiEhIezatYuoqChiY2MJCwtjw4YNREVFsXjxYtt9nL7s6+tLXFwcgwYNwmq18sEHHwAwdOhQDh48SHBwMEVFRfj6+nL48GEA3n33XUJDQwkODmbMmDFkZWXZ9jt//nwiIiLo168fERERFBYWXujTKCIiItXooi7VcPz4cSwWC//5z3/YsGEDc+fOJTQ0lA0bNrBt2zYSEhKoW7cu48aNY/ny5URGRpYbv27dOnbv3s3GjRs5efIkt99+OzfddBPp6elnHZ+WlkZiYiIWi4UPP/yQ7du3Ex8fT506ddi6detZM//++++sXbuWffv2MWjQIIKCgoiJiSE6OpqkpKRy237zzTe89dZbJCQk4OXlxYwZM5gzZw4vvvgiUDbT8/7773PFFVcwZMgQkpOT+de//nUxT2Wt4u3tYZexNUH5Lo3Z80n1utTX3yy/P2bIYYYMYI4cF9XgFBcXEx4eDkDHjh05ePAgAFu2bOGOO+6gfv36AISHh7Ns2bIKDc62bdsICgrC1dUVV1dX1q9fT7169YiLizvr+ICAACyW/0069ezZkzp16pxX5iFDhgDQunVrWrVqxZ49e6hbt+4Zt92yZQtBQUF4eXkBcNddd/Hoo4/a1gcEBNCwYUMA2rZty6FDh84rQ2135EjeRY3z9va46LE1Qfkujb3ymeENWMpcyutvlt9vM+QwQwZ75Kisli/qHBxnZ2dbE2KxWCgtLQUgKysLT09P23aenp4cO3aswvjjx4/ToEED23L9+vVxcnI65/jT151p+Wz+ud/c3NxKt83KyiqXr0GDBuVyeHj878l0dnampKTkvHOIiIhI9avSk4ybNGlCdna2bTk7O/uMJ+02atSI48eP25aPHj3KiRMnznv8mZzeaJ0ae7rT7y87O/uszdGl5BARERH7q9IGJyAggHXr1lFYWEhxcTGrVq0iICCgwnZWq5XExESKiorIz89n2LBh/PTTT+c9/ky8vb354YcfAEhPT+frr78utz4xMRGAX3/9lf3799O1a1dcXFwoKCiguLi43LZ9+/YlOTnZ1hStXLnyvHOIiIiI/V3UOTiVCQkJ4aeffiI8PBzDMOjRowcjR46ssF1oaCg//vgjAwcOpE6dOtx55510794dwzDOa/yZ3H333YwbN46BAwfSoUMHgoKCyq1v3LgxgwYNIjc3l+joaDw9PfH19cXT05NevXqxZs0a27ZdunRh9OjR3HfffZSWltK+fXuee+65S3puREREpOY4GYZh2DtEdfP19WXr1q00a9bMbhnCJq+1232bxdtR1osaZ5YT5yqjfJfmcjrJWHVcPS72vQHM8/tthhxmyGCPHFV6krGIiIiImanBEREREYdTpefgmNWPP/5o7wgiIiJSgzSDIyIiIg5HDY6IiIg4nFpxiMoM/jNnkCnObq+MWc6+FzEzs9SxWerVLDlEzkQzOCIiIuJw1OCIiIiIw1GDIyIiIg6nVnyTsYiIiNQumsERERERh6MGR0RERByOGhwRERFxOGpwRERExOGowRERERGHowZHREREHI4aHBEREXE4uhZVNYuJieHbb7/FycmJqVOn0qVLF3tHAuCnn34iMjKSBx54gOHDh3Po0CGefPJJSkpK8Pb25qWXXsLNzc0u2WbPns3OnTspLi7mkUceoXPnzqbJVlhYSFRUFMeOHePkyZNERkbSrl070+Q75a+//uK2225j7Nix9OzZ0zT50tLSiIyM5JprrgGgbdu2PPTQQ6bJdzb2rGWz1Ku9a9NM9WfvGjNTLa1bt46lS5fi4uLChAkTaNu2rTlq2pBqk5KSYowePdowDMP4+eefjTvvvNPOicrk5+cbw4cPN6Kjo424uDjDMAwjKirKWL9+vWEYhjFr1izjvffes0u27du3Gw899JBhGIaRlZVlBAQEmCabYRhGYmKi8eabbxqGYRgZGRnGwIEDTZXvlFdeecUIDw83Vq9ebap8KSkpxgsvvFDuNjPlq4w9a9ks9WqG2jRT/dm7xsxSS1lZWcbAgQONvLw8IzMz04iOjjZNTesQVTXavn07AwYMAOC6664jNzeXEydO2DkVuLm5sWTJEnx8fGy3paSk0L9/fwD69+/P9u3b7ZLtxhtvZN68eQB4enpSWFhommwAoaGhPPzwwwAcOnSIpk2bmiofwK+//sovv/xC3759AfO8tgD5+fkVbjNTvsrYs5bNUq9mqE2z1J8ZaswstbR9+3Z69uzJFVdcgY+PDzNmzDBNTavBqUZHjx6lUaNGtmUvLy+OHDlix0RlXFxcqFu3brnbCgsLbVOI3t7edsvp7OxM/fr1AVi1ahW33nqrabKdbujQoTz++ONMnTrVdPlmzZpFVFSUbdlM+QoKCti5cycPPfQQ9913H19++aWp8lXGnrVslno1U23au/7MUGNmqaWMjAwMw+Cxxx5j2LBhbN++3TQ1rXNwqpHxj8t8GYaBk5OTndKc3em5/pnbHjZv3kx8fDxvv/02QUFBttvNkA1g5cqVfP/99zzxxBOmeu4++ugjunXrRsuWLW23mSlfu3btGDt2LP379+e3334jIiKC4uJi23p756uM2WrZnq+pGWrTnvVnlhozUy1lZmaycOFCDh48yMiRI03znqMGpxo1bdqUo0eP2pb//PNPmjRpYsdElatXrx5//fUXdevWJTMzs9x0eE379NNPef3111m6dCkeHh6mypaWloaXlxdXXnkl7du3p6SkxFT5tmzZQnp6Olu2bOHw4cO4ubmZKl+bNm1o06YNAK1ataJJkyYcOnTINPkqY7Zattdrau/aNEP9maXGzFJLXl5e+Pn54eLiwtVXX427uzvOzs6mqGkdoqpGvXr1YuPGjQB89913+Pj4cMUVV9g51ZndcssttqybNm2iT58+dsmRl5fH7NmzeeONN2jYsKGpsgGkpqby9ttvA2WHLQoKCkyV79VXX2X16tV8+OGH3HXXXURGRpoqX3x8PO+++y4AR44c4dixY4SHh5smX2XMVsv2eE3NUJtmqD+z1JhZaql37958+eWXlJaWkpWVZar3RCfDrHPCDuLll18mNTUVJycnnn32Wdq1a2fvSKSlpTFr1iz++OMPXFxcaNq0KS+//DJRUVGcPHmSq666itjYWFxdXWs827///W8WLFhAq1atbLfNnDmT6Ohou2eDso+GPv3007a/lMaNG0enTp146qmnTJHvdAsWLKB58+b07t3bNPlycnJ4/PHHKSgooKioiHHjxtG+fXvT5Dsbe9WyWerVDLVptvqzZ42ZqZZWrlxJYmIihYWFjBkzhs6dO5uiptXgiIiIiMPRISoRERFxOGpwRERExOGowRERERGHowZHREREHI4aHBEREXE4anDksvPhhx/adfy57N69m1GjRl3QmD179hAaGkpoaCh79uyx3Z6bm8vgwYNNcQ0zETk/nTt3ZuvWree1rdVqZcWKFdWcqHZSgyOXlZKSEmbPnm238eejS5cuvPXWWxc0ZvHixcTExBATE8PixYttt7/66qs8/PDDpv2CSKl9zvUf8o4dOxg6dCjdu3enb9++zJ49u9wlBGqDPXv2EBAQUCX7SklJYffu3VWyr9pGDY5cViIiIsjLyyM4OJj09HQyMzN59NFHCQoKIigoyPZXU3FxMdHR0QQFBREYGMi4ceM4ceJEhfH/tGjRIoKCgujXrx8vvPACJSUlAIwYMYK5c+cSEhLCrl27iIqKIjY2lrCwMDZs2FBuHykpKQQGBgJlXwQ2ffp02zVj7rzzTv78888K97tv3z46dOhAhw4d2LdvHwB79+5l//79hIaGVulzKFJdDh48yOjRowkNDSUlJYU33niDdevW8c4779g72mVr2bJlanAukhocuazExMTg7OxMUlISLVu25JlnnqFdu3Zs3LiRN998kyeffJLjx4/z2WefkZ6eTlJSEps2beK6667j66+/rjD+dElJSWzYsIH4+HiSk5NJT0/ngw8+sK1PS0sjMTGR7t27A7B9+3bi4+MJCQk5a+akpCSmTp3K5s2b8fLyYvXq1RW2cXJywjAMSkpKsFgsGIZBTEwMY8aMYdKkSTz66KPs3bu3Cp5Bkepz9OhRwsPDGTlyJK6urvj6+mK1WtmxY8cZt4+KimL69OnMnDmTm266iZ49e7J8+XLb+tzcXKZMmUKfPn3o0aMHo0aN4ueff7at9/X1ZdmyZfTp04cFCxaQkpJCt27d+Pjjj7Farfj5+REbG8sPP/zAHXfcgZ+fH5GRkRQVFVXI8vjjj/Piiy/alt988018fX05duwYUHbRyB49etguS7Bw4UICAwPp2rUrd9xxR7kmxNfXl08++QSArKws7r//frp06UJYWBiffvopvr6+/PTTT7btCwsLmTRpEn5+fgwcOJDPP/8cgIcffphPPvmE2NhYhg8fDsCSJUuwWq107dqV/v37ExcXd6EvU62hBkcuWwUFBWzdupVhw4YBcM011+Dv78/WrVtp3Lgxv/76K8nJyRQWFvLYY4+d83ooGzZsICwsDA8PD1xcXLjrrrvYtGmTbX1AQAAWy/9KpmfPntSpU+ecOW+44QaaN2+Ok5MT7du359ChQxW26dChA6mpqXz11Vd07NiR+Ph4unfvzrZt2+jXrx/PP/88M2fOPN+nRsQuunTpwrRp08rddvjwYZo2bVrpmPXr19O2bVs+//xzxo0bx0svvcTx48cBiI6OJiMjgzVr1vDJJ5/g7e3No48+aptZBdi4cSMJCQmMGzcOKLucw2effUZiYiIxMTEsX76cl19+maVLl7J69Wq2bdtmaz5Od/PNN7Nr1y7b8o4dO2jVqhU7d+4E4Oeff+avv/6ie/fuvPvuu6xdu5Y33niD1NRU7r33Xu6//36ys7Mr7HfGjBmcPHmSrVu3snDhQubNm1dhm/j4eEaOHMmXX36Jv78/06dPB8qamebNmzNlyhRWrFjBrl27WLBgAa+99hrffvstc+fOZcGCBfz444+VPr+1mRocuWzl5eVhGAYjR44kODiY4OBg0tLSyM3NpUuXLkRHRxMXF0evXr2YPHkyubm559xfXFycbV+zZs3i5MmTtvWenp7ltv/ncmU8PDxsPzs7O5d7cz5l/PjxzJs3j0WLFjF8+HDef/99xowZw969e+nYsSNNmzbljz/+OK/7EzGL//73v+zYsYOIiIhKt2nWrBnh4eG4uroSHBxMcXExBw4cICcnh02bNjFhwgSaNGlC/fr1mThxIhkZGXz33Xe28SEhIXh7e+Pk5ASUzbQMGzaMevXqYbVagbLzhpo0aULr1q259tpr2b9/f4UcPXv25IcffqCgoICSkhJ2797N3XffTWpqKlB2oU9/f3/c3NxYtWoV999/P61bt8bV1ZV77rmHFi1akJSUVG6fpaWlbN68mQceeIBGjRpxzTXXcO+991a47759+9KtWzfq1KlDcHAw+/fv5++//66wXV5eHgD169cHyhrKL7/8El9f37O+DrWVi70DiFwsLy8vnJ2dWb16Ne7u7hXWn2pUsrOzmTp1Km+99RZ33XVXpfvz8fHBarXapoJrUqtWrWyf7po2bRpjxoyhfv36nH6puNNnj0TMbvXq1bz44ovMnz+fa6+9ttLtWrRoYfu5bt26QNkszB9//IFhGFx33XW29U2bNsXd3Z1Dhw7RuXNnAJo3b15hn82aNQOwzbCePoPk5uZW7g+XU5o3b06zZs3YvXs37u7uXHPNNdxyyy08/fTTAOzcuZObb74ZgAMHDjBz5kxmzZplG28YRoXZ2ezsbIqKisplbN++/TmfA8MwKCoqqnCByp49e3LLLbcQEhLCTTfdRO/evRk8eDCNGjWqsE/RDI5cZlxdXSktLeXEiRO4uLhw6623snLlSqDsOPaUKVM4dOgQq1evZtGiRQA0bNiQ1q1bVxj/T1arlbVr11JYWAiUXSF3zZo1NfTIyuzevZvMzEwGDhwIQJs2bdizZw/p6el6E5PLxuLFi22Hhc51aPhcjfupmZnTnT674ezsfM4x5/vHwanDVKdma9q2bUtGRgb5+fmkpqZyyy23AGVNyOzZs9mzZ4/tX1paGhMnTiy3v1N/oJzeqJwpy5ke45m4ubnx+uuvEx8fj7+/PwkJCYSGhp7xAxOiBkcuM97e3vj7+9OvXz927drF888/z44dOwgODmbw4MG0bNmSK6+8kv79+7N3714GDhxISEgIv/zyCxERERXGny4wMJB+/foxePBggoOD+fjjj+ndu3eNPbbS0lJiY2NtfzEC3H///SxbtoxRo0YxYcKEGssicrHi4uJYuXIlH3zwge2E/IvRokULnJyc+OWXX2y3ZWZmkp+fz9VXX10VUSs41eCkpKTg7++PxWKhY8eOrF+/nr/++osOHToAcPXVV1c47yUjI6PC/ho2bIizs3O5w8vff//9RecrLi4mNzeXdu3aMXbsWD766CM8PDxITk6+6H06Mh2iksuKxWLhvffeK3fb66+/XmG7hg0blvs+mdP9c/wpTk5OREZGEhkZWWHdPz+pcLYTfnv06GF7wxk/fny5df9cPp3FYin3qS0oe5P/6KOPKh0jYiYZGRm88sorrFix4qyHpc5HgwYNCAoKYt68ebz66qu4ubnx0ksv0bZtWzp16lQ1gf/h5ptvtn2SKiYmBgA/Pz+WL19Ojx49bLMv9957L7Nnz6ZPnz74+fmxZcsWJk+eTEJCgm22GMpml3r16sU777zDjTfeSHZ29gV/0WidOnU4cOAAeXl5vP/++6xfv55FixbRokULfvvtN3Jycqqt4bvcqcEREZELEhsbW+78Eyj7Q+Obb76hsLCQoUOHllt31VVXsXHjxgu+n2effZbnn3+esLAwSktLufHGG1m6dOl5H9K5UN7e3nh5eWEYBo0bNwage/fuLFy4kPvuu8+23ZAhQzh8+DATJ04kNzeXa6+9ljlz5pRrbk555plneOqppwgICKBt27ZERkYyevTo8z5sds899zB37ly++OILEhISOHz4MHfffTf5+fl4e3szatQoBgwYUDVPgINxMk4/i1FERESqVFFREW5ubgB8/fXXDB06lNTU1HKfsJSqp3NwREREqsnUqVMZNWoUOTk55OXlsWTJEvz8/NTc1AA1OCIiItXkiSeeoHHjxgQGBjJgwABKSkp46aWX7B2rVtAhKhEREXE4msERERERh6MGR0RERByOGhwRERFxOGpwRERExOGowRERERGH8/8AbXEIASdPFucAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x144 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_,axs=plt.subplots(1,2,figsize=(8,2),sharey=True)\n",
    "axs[0].barh(means.index,means[\"test err\"]*100)\n",
    "axs[0].set_xlabel(\"test err in %\",fontsize=12)\n",
    "axs[0].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "axs[1].barh(means.index,means[\"L2-norm\"])\n",
    "axs[1].set_xlabel(\"L2 norm weights\",fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rndlabels.png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the L2 norms of the weight matrices we observe that SGD has an implicit regularising effect. Indeed, the local minimum reached by SGD for the unadulterated training set has significantly smaller L2 norm than the weights of the other minima. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
