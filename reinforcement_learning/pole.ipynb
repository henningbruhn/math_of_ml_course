{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0aa93e-7eab-4ba8-b202-dee647985877",
   "metadata": {},
   "source": [
    "# Pole balancing\n",
    "\n",
    "We consider the classic pole balancing task due to Sutton & Barto (see also lecture notes). \n",
    "\n",
    "Each state is determined by four values:\n",
    "\n",
    "|Num |    Observation |              Min  |                   Max |\n",
    "|----------|--------|-------------|---------------|\n",
    "|0|       Cart Position|             -4.8 |                   4.8|\n",
    "|1|       Cart Velocity      |       -Inf         |           Inf|\n",
    "|2|       Pole Angle        |        -0.418 rad (-24 deg) |   0.418 rad (24 deg)|\n",
    "|3|       Pole Angular Velocity |    -Inf           |         Inf|\n",
    "\n",
    "There are only two actions possible\n",
    "\n",
    "|Num   |Action|\n",
    "|-------|--------|\n",
    "|0|  Push cart to the left|\n",
    "|1|  Push cart to the right|\n",
    "\n",
    "\n",
    "The pole balancing task is implemented in the package <code>gym</code> that you need to install. The package provides a framework for reinforcement tasks and a number of fun tasks. Check it out here: http://gym.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b68f494-656b-4eac-bfbd-7b5635a20df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install gym in colab via\n",
    "# !pip install gym\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac376b5-78ab-43f4-98bf-845456683ba9",
   "metadata": {},
   "source": [
    "We initialise the task with <code>gym.make</code>. Unfortunately, it's not easy to get information on the task. We only get to the documentation when we find out where explicitely the task is defined. (<code>help(env)</code> does **not** work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1532e3a1-ada0-4553-b3e1-d41f5922b2a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gym.envs.classic_control.cartpole in gym.envs.classic_control:\n",
      "\n",
      "NAME\n",
      "    gym.envs.classic_control.cartpole\n",
      "\n",
      "DESCRIPTION\n",
      "    Classic cart-pole system implemented by Rich Sutton et al.\n",
      "    Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
      "    permalink: https://perma.cc/C9ZM-652R\n",
      "\n",
      "CLASSES\n",
      "    gym.core.Env(builtins.object)\n",
      "        CartPoleEnv\n",
      "    \n",
      "    class CartPoleEnv(gym.core.Env)\n",
      "     |  Description:\n",
      "     |      A pole is attached by an un-actuated joint to a cart, which moves along\n",
      "     |      a frictionless track. The pendulum starts upright, and the goal is to\n",
      "     |      prevent it from falling over by increasing and reducing the cart's\n",
      "     |      velocity.\n",
      "     |  \n",
      "     |  Source:\n",
      "     |      This environment corresponds to the version of the cart-pole problem\n",
      "     |      described by Barto, Sutton, and Anderson\n",
      "     |  \n",
      "     |  Observation:\n",
      "     |      Type: Box(4)\n",
      "     |      Num     Observation               Min                     Max\n",
      "     |      0       Cart Position             -4.8                    4.8\n",
      "     |      1       Cart Velocity             -Inf                    Inf\n",
      "     |      2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
      "     |      3       Pole Angular Velocity     -Inf                    Inf\n",
      "     |  \n",
      "     |  Actions:\n",
      "     |      Type: Discrete(2)\n",
      "     |      Num   Action\n",
      "     |      0     Push cart to the left\n",
      "     |      1     Push cart to the right\n",
      "     |  \n",
      "     |      Note: The amount the velocity that is reduced or increased is not\n",
      "     |      fixed; it depends on the angle the pole is pointing. This is because\n",
      "     |      the center of gravity of the pole increases the amount of energy needed\n",
      "     |      to move the cart underneath it\n",
      "     |  \n",
      "     |  Reward:\n",
      "     |      Reward is 1 for every step taken, including the termination step\n",
      "     |  \n",
      "     |  Starting State:\n",
      "     |      All observations are assigned a uniform random value in [-0.05..0.05]\n",
      "     |  \n",
      "     |  Episode Termination:\n",
      "     |      Pole Angle is more than 12 degrees.\n",
      "     |      Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
      "     |      the display).\n",
      "     |      Episode length is greater than 200.\n",
      "     |      Solved Requirements:\n",
      "     |      Considered solved when the average return is greater than or equal to\n",
      "     |      195.0 over 100 consecutive trials.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CartPoleEnv\n",
      "     |      gym.core.Env\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Override close in your subclass to perform any necessary cleanup.\n",
      "     |      \n",
      "     |      Environments will automatically close() themselves when\n",
      "     |      garbage collected or when the program exits.\n",
      "     |  \n",
      "     |  render(self, mode='human')\n",
      "     |      Renders the environment.\n",
      "     |      \n",
      "     |      The set of supported modes varies per environment. (And some\n",
      "     |      environments do not support rendering at all.) By convention,\n",
      "     |      if mode is:\n",
      "     |      \n",
      "     |      - human: render to the current display or terminal and\n",
      "     |        return nothing. Usually for human consumption.\n",
      "     |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      "     |        representing RGB values for an x-by-y pixel image, suitable\n",
      "     |        for turning into a video.\n",
      "     |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      "     |        terminal-style text representation. The text can include newlines\n",
      "     |        and ANSI escape sequences (e.g. for colors).\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Make sure that your class's metadata 'render.modes' key includes\n",
      "     |            the list of supported modes. It's recommended to call super()\n",
      "     |            in implementations to use the functionality of this method.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (str): the mode to render with\n",
      "     |      \n",
      "     |      Example:\n",
      "     |      \n",
      "     |      class MyEnv(Env):\n",
      "     |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      "     |      \n",
      "     |          def render(self, mode='human'):\n",
      "     |              if mode == 'rgb_array':\n",
      "     |                  return np.array(...) # return RGB frame suitable for video\n",
      "     |              elif mode == 'human':\n",
      "     |                  ... # pop up a window and render\n",
      "     |              else:\n",
      "     |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |      Resets the environment to an initial state and returns an initial\n",
      "     |      observation.\n",
      "     |      \n",
      "     |      Note that this function should not reset the environment's random\n",
      "     |      number generator(s); random variables in the environment's state should\n",
      "     |      be sampled independently between multiple calls to `reset()`. In other\n",
      "     |      words, each call of `reset()` should yield an environment suitable for\n",
      "     |      a new episode, independent of previous episodes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          observation (object): the initial observation.\n",
      "     |  \n",
      "     |  seed(self, seed=None)\n",
      "     |      Sets the seed for this env's random number generator(s).\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Some environments use multiple pseudorandom number generators.\n",
      "     |          We want to capture all such seeds used in order to ensure that\n",
      "     |          there aren't accidental correlations between multiple generators.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          list<bigint>: Returns the list of seeds used in this env's random\n",
      "     |            number generators. The first value in the list should be the\n",
      "     |            \"main\" seed, or the value which a reproducer should pass to\n",
      "     |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      "     |            this won't be true if seed=None, for example.\n",
      "     |  \n",
      "     |  step(self, action)\n",
      "     |      Run one timestep of the environment's dynamics. When end of\n",
      "     |      episode is reached, you are responsible for calling `reset()`\n",
      "     |      to reset this environment's state.\n",
      "     |      \n",
      "     |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          action (object): an action provided by the agent\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          observation (object): agent's observation of the current environment\n",
      "     |          reward (float) : amount of reward returned after previous action\n",
      "     |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      "     |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gym.core.Env:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |      Support with-statement for the environment.\n",
      "     |  \n",
      "     |  __exit__(self, *args)\n",
      "     |      Support with-statement for the environment.\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from gym.core.Env:\n",
      "     |  \n",
      "     |  unwrapped\n",
      "     |      Completely unwrap this env.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          gym.Env: The base non-wrapped gym.Env instance\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gym.core.Env:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gym.core.Env:\n",
      "     |  \n",
      "     |  action_space = None\n",
      "     |  \n",
      "     |  observation_space = None\n",
      "     |  \n",
      "     |  reward_range = (-inf, inf)\n",
      "     |  \n",
      "     |  spec = None\n",
      "\n",
      "FILE\n",
      "    /home/henning/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"CartPole-v1\")\n",
    "help(gym.envs.classic_control.cartpole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d46b78-3924-47b3-953e-a103133dc17d",
   "metadata": {},
   "source": [
    "From the documentation we learn that there are two actions possible, and we introduce variables to better distinguish between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e29267-e730-4c9a-a99c-c9788966ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define two variable so that we can better distinguish between the actions\n",
    "push_left=0\n",
    "push_right=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bc8f0-17e4-4c79-80ae-2764c72f9695",
   "metadata": {},
   "source": [
    "Next, reset the environment and observe the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bcff24-e524-4f68-8e8e-157c00a7fd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01698376,  0.01610257, -0.02270971, -0.00659515], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a84c66-d66d-4f30-9691-7d4bf886b8be",
   "metadata": {},
   "source": [
    "Let's choose an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890678ce-6766-4c73-a994-54ad36599203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01730581, -0.17868645, -0.02284162,  0.27883697], dtype=float32),\n",
       " 1.0,\n",
       " False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, reward, done, info = env.step(push_left)\n",
    "new_state,reward,done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f2f46-565f-4a25-8899-180d9028c620",
   "metadata": {},
   "source": [
    "## Let's try out a simple strategy\n",
    "\n",
    "We set up a simple strategy: Whenever the pole leans to the left, push left; whenever it leans to the right, push right. We then test the strategy by playing 1000 times and computing the mean total reward. We set the maximum length of an episode to 300: If you can balance the pole for that long you probably can do so indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96a4d79-8837-4b5b-aa8d-322e804c15a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.058"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_policy(state):\n",
    "    angle=state[2]\n",
    "    return push_left if angle <0 else push_right\n",
    "\n",
    "def play_single_episode(policy,env,max_episode_length=300):\n",
    "    total_reward=0\n",
    "    state=env.reset()\n",
    "    for step in range(max_episode_length):\n",
    "        action=policy(state)\n",
    "        state,reward,done,info=env.step(action)\n",
    "        total_reward+=reward\n",
    "        if done: # pole fell over, episode ended\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def play_many_episodes(policy,env,repeats,max_episode_length=300):\n",
    "    total_rewards=[]\n",
    "    for i in range(repeats):\n",
    "        total_rewards.append(play_single_episode(policy,env,max_episode_length))\n",
    "    return sum(total_rewards)/len(total_rewards)\n",
    "\n",
    "play_many_episodes(basic_policy,env,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2025393-913f-48b9-9fc9-62d10df8f471",
   "metadata": {},
   "source": [
    "Could be better! Can you come up with a better policy? If you want to experiment, you should consult the [documentation](http://gym.openai.com/docs/), where, in particular, you can find out how to visualise episodes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
