{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training errors and test errors\n",
    "\n",
    "How do we evaluate how well a classification algorithm performs? So far we've only looked at the error on the training set. However, we know the training set. What we really want is good performance on *unknown* data? How can that be measured?\n",
    "\n",
    "Let's look at the MNIST dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import zero_one_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data from openml.org\n",
    "# see https://en.wikipedia.org/wiki/MNIST_database\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "# the classes are accessed via mnist.target\n",
    "# however, they are stored as strings and we want the digits to be stored as integers\n",
    "# we use astype to convert them\n",
    "mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "# newer versions of scikit-learn use pandas dataframe to store the data\n",
    "# we convert these to a simple numpy array \n",
    "X, y = np.array(mnist[\"data\"]), np.array(mnist[\"target\"]) \n",
    "X.shape  # X.shape returns the dimensions of the array X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, have a look at a sample, just to get feeling with what we're dealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE8klEQVR4nO3dsW+NbQCHYa1vQYSBUVkNRCJGq8Fg6mgyMJ2/w2T2NwiisRgtImxiYZBYmhwDkZjaRHK+WdI+/b721LnbXtfYX954BnefxJtzLM1ms2NAz/KiDwBsTZwQJU6IEidEiROi/tlh90+5sP+WtvqhmxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHqn0UfgP9nNpsN9+l0OtyfPn063J89e7bt9uXLl+Gz79+/H+4rKyvDnT+5OSFKnBAlTogSJ0SJE6LECVHihCjvORdgfX19221tbW347JMnT4b7mzdvdnOk/+TUqVPD/eTJk/v2Zx9Fbk6IEidEiROixAlR4oQocUKUOCHKe85d+Pjx43B/+PDhcH/x4sW22+bm5vDZS5cuDffJZDLcf//+PdwfP3687Xbr1q3hs+fOnRvu/D9uTogSJ0SJE6LECVHihChxQpQ4IepIvud8/fr1cL93795w//bt23Df2NgY7vfv3992u3v37vDZ69evD/edPlP54cOH4T56z3nlypXhs8yXmxOixAlR4oQocUKUOCFKnBAlTog6ku85v3//PtyvXbs23Hf6/tbV1dXhfufOnW235eXu78sTJ04s+ghHSvdvAhxx4oQocUKUOCFKnBAlTohams1mo304cvDcvn17uL969Wrb7efPn8Nnz549u5sjcezY0lY/dHNClDghSpwQJU6IEidEiROixAlRR/IjY0fZdDpd9BH4j9ycECVOiBInRIkTosQJUeKEKHFClPec/OHGjRvbbqdPn/6LJ8HNCVHihChxQpQ4IUqcECVOiBInRHnPecisr68P90+fPg330X9PePz48V2did1xc0KUOCFKnBAlTogSJ0SJE6LECVHecx4ya2trw31zc3O4TyaTOZ6GvXBzQpQ4IUqcECVOiBInRIkTorxKOWTevn073JeXx7+PL168OM/jsAduTogSJ0SJE6LECVHihChxQpQ4Icp7zkNmOp0O96tXrw73lZWVeR6HPXBzQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRPs95wPz69Wu4v3v3brjfvHlznsdhH7k5IUqcECVOiBInRIkTosQJUV6lHDAvX74c7hsbG8N9MpnM8zjsIzcnRIkTosQJUeKEKHFClDghSpwQ5T3nAfP8+fM9PX/hwoU5nYT95uaEKHFClDghSpwQJU6IEidEiROivOc8ZM6cOTPcz58//5dOwl65OSFKnBAlTogSJ0SJE6LECVHihKil2Ww22ocjf9/ly5eH+07fW/v169d5Hof5WNrqh25OiBInRIkTosQJUeKEKHFClDghyuc5Yx49ejTcP3/+PNwfPHgwz+OwQG5OiBInRIkTosQJUeKEKHFClFcpMT9+/NjT86urq3M6CYvm5oQocUKUOCFKnBAlTogSJ0SJE6J8NSYsnq/GhINEnBAlTogSJ0SJE6LECVHihKidPs+55fsXYP+5OSFKnBAlTogSJ0SJE6LECVH/An5CixuGdyRAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot a data point\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = 'binary')\n",
    "    plt.axis(\"off\")\n",
    "plot_digit(X[42,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test set\n",
    "\n",
    "In machine learning, we normally split the available data into two (sometimes more) parts: a training set and a test set. The training set is used for learning, ie, to *train* the classifier. With the test set we evaluate the classifier. \n",
    "\n",
    "There are 70000 samples in the data set. Normally, we'd take the larger part, say 60000 samples, for the training set and the rest for the test set. However, some of the algorithms below are really quite slow to train. So let's take a substantially smaller training set, just because I'm impatient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pick a large random subset as training set\n",
    "# the rest makes up the test set\n",
    "X, y = sklearn.utils.shuffle(X,y)  # make sure the data is in random order\n",
    "train_size=10000 # we pick 10000 of the 70000 samples for the training set, the rest goes into the test set\n",
    "X_train, X_test, y_train, y_test = X[:train_size], X[train_size:], y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of classes in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 945., 1107.,  983., 1023.,  941.,  919., 1012., 1087.,  947.,\n",
       "        1036.]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOs0lEQVR4nO3cb6hdV53G8e8ziVZbKSb0tqRJmEQIalqQOpdOtSAyEZqZFtMXU4mgEyRDYIhaRZDEN30V6AsRZZgWQlubwU4zIRYaHEctUZGBIZ3bVtAklgbjJNfG5jqOf/BFNfE3L+4uc7i9aXPPvvec5q7vB8Lee521z/ptEp6zss7ZO1WFJKkNfzbuAiRJo2PoS1JDDH1JaoihL0kNMfQlqSErx13A67nuuutqw4YN4y5Dkq4ozzzzzC+ramJu+xs+9Dds2MDU1NS4y5CkK0qS/56v3eUdSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyBv+jtwr0YY9/7bkY/zs/juXfAxJy48zfUlqiKEvSQ1xeUfSZVvqpUuXLZeeM31JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqId+RK0mtYbnchO9OXpIY409ei8HHS0pXBmb4kNeR1Z/pJHgHuAs5X1c1d22rgX4ENwM+Aj1TV/3av7QV2AheBT1fVt7v2vwAeBd4KfBO4t6pqcS9HasNyW2fW6FzOTP9RYOuctj3A0araBBztjkmyGdgO3NSd80CSFd05DwK7gE3dn7nvKUlaYq8b+lX1A+BXc5q3AQe6/QPA3QPtB6vq5ao6DZwCbk2yBri2qv6zm93/88A5kqQRGXZN/4aqOgfQba/v2tcCZwf6TXdta7v9ue3zSrIryVSSqZmZmSFLlCTNtdhf5GaetnqN9nlV1f6qmqyqyYmJiUUrTpJaN2zov9Qt2dBtz3ft08D6gX7rgBe79nXztEuSRmjY3+kfAXYA93fbJwfa/yXJl4Abmf3C9umqupjkd0luA44Bfwf8Y6/KNS9/1SHptVzOTzYfBz4IXJdkGriP2bA/lGQncAa4B6Cqjic5BJwALgC7q+pi91b/wP//ZPPfuz+SpBF63dCvqo9e4qUtl+i/D9g3T/sUcPOCqpMkLSrvyJWkhhj6ktQQQ1+SGmLoS1JDlvWjlf35orQ8+OjuxbOsQ19t8MNdunwu70hSQwx9SWqIoS9JDXFNXxqS3yXoSuRMX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ3qFfpLPJjme5MdJHk/yliSrkzyV5IVuu2qg/94kp5I8n+SO/uVLkhZi6NBPshb4NDBZVTcDK4DtwB7gaFVtAo52xyTZ3L1+E7AVeCDJin7lS5IWou/yzkrgrUlWAlcDLwLbgAPd6weAu7v9bcDBqnq5qk4Dp4Bbe44vSVqAoUO/qn4OfBE4A5wDflNV3wFuqKpzXZ9zwPXdKWuBswNvMd21vUqSXUmmkkzNzMwMW6IkaY4+yzurmJ29bwRuBK5J8rHXOmWetpqvY1Xtr6rJqpqcmJgYtkRJ0hx9lnc+BJyuqpmq+iPwBPB+4KUkawC67fmu/zSwfuD8dcwuB0mSRqRP6J8BbktydZIAW4CTwBFgR9dnB/Bkt38E2J7kqiQbgU3A0z3GlyQt0MphT6yqY0kOA88CF4DngP3A24BDSXYy+8FwT9f/eJJDwImu/+6qutizfknSAgwd+gBVdR9w35zml5md9c/Xfx+wr8+YkqTheUeuJDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGtIr9JO8PcnhJD9JcjLJ+5KsTvJUkhe67aqB/nuTnEryfJI7+pcvSVqIvjP9rwDfqqp3Ae8BTgJ7gKNVtQk42h2TZDOwHbgJ2Ao8kGRFz/ElSQswdOgnuRb4APAwQFX9oap+DWwDDnTdDgB3d/vbgINV9XJVnQZOAbcOO74kaeH6zPTfAcwAX03yXJKHklwD3FBV5wC67fVd/7XA2YHzp7s2SdKI9An9lcB7gQer6hbg93RLOZeQedpq3o7JriRTSaZmZmZ6lChJGtQn9KeB6ao61h0fZvZD4KUkawC67fmB/usHzl8HvDjfG1fV/qqarKrJiYmJHiVKkgYNHfpV9QvgbJJ3dk1bgBPAEWBH17YDeLLbPwJsT3JVko3AJuDpYceXJC3cyp7nfwp4LMmbgZ8Cn2D2g+RQkp3AGeAegKo6nuQQsx8MF4DdVXWx5/iSpAXoFfpV9UNgcp6Xtlyi/z5gX58xJUnD845cSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SG9A79JCuSPJfkG93x6iRPJXmh264a6Ls3yakkzye5o+/YkqSFWYyZ/r3AyYHjPcDRqtoEHO2OSbIZ2A7cBGwFHkiyYhHGlyRdpl6hn2QdcCfw0EDzNuBAt38AuHug/WBVvVxVp4FTwK19xpckLUzfmf6Xgc8Dfxpou6GqzgF02+u79rXA2YF+013bqyTZlWQqydTMzEzPEiVJrxg69JPcBZyvqmcu95R52mq+jlW1v6omq2pyYmJi2BIlSXOs7HHu7cCHk/wN8Bbg2iRfA15KsqaqziVZA5zv+k8D6wfOXwe82GN8SdICDT3Tr6q9VbWuqjYw+wXtd6vqY8ARYEfXbQfwZLd/BNie5KokG4FNwNNDVy5JWrA+M/1LuR84lGQncAa4B6Cqjic5BJwALgC7q+riEowvSbqERQn9qvo+8P1u/3+ALZfotw/YtxhjSpIWzjtyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JChQz/J+iTfS3IyyfEk93btq5M8leSFbrtq4Jy9SU4leT7JHYtxAZKky9dnpn8B+FxVvRu4DdidZDOwBzhaVZuAo90x3WvbgZuArcADSVb0KV6StDBDh35VnauqZ7v93wEngbXANuBA1+0AcHe3vw04WFUvV9Vp4BRw67DjS5IWblHW9JNsAG4BjgE3VNU5mP1gAK7vuq0Fzg6cNt21zfd+u5JMJZmamZlZjBIlSSxC6Cd5G/B14DNV9dvX6jpPW83Xsar2V9VkVU1OTEz0LVGS1OkV+knexGzgP1ZVT3TNLyVZ072+BjjftU8D6wdOXwe82Gd8SdLC9Pn1ToCHgZNV9aWBl44AO7r9HcCTA+3bk1yVZCOwCXh62PElSQu3sse5twMfB36U5Idd2xeA+4FDSXYCZ4B7AKrqeJJDwAlmf/mzu6ou9hhfkrRAQ4d+Vf0H86/TA2y5xDn7gH3DjilJ6sc7ciWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGjLy0E+yNcnzSU4l2TPq8SWpZSMN/SQrgH8C/hrYDHw0yeZR1iBJLRv1TP9W4FRV/bSq/gAcBLaNuAZJalaqanSDJX8LbK2qv++OPw78ZVV9ck6/XcCu7vCdwPNDDnkd8Mshz71Sec1taO2aW7te6H/Nf15VE3MbV/Z4w2FknrZXfepU1X5gf+/Bkqmqmuz7PlcSr7kNrV1za9cLS3fNo17emQbWDxyvA14ccQ2S1KxRh/5/AZuSbEzyZmA7cGTENUhSs0a6vFNVF5J8Evg2sAJ4pKqOL+GQvZeIrkBecxtau+bWrheW6JpH+kWuJGm8vCNXkhpi6EtSQ5Zl6Lf2qIck65N8L8nJJMeT3DvumkYlyYokzyX5xrhrGYUkb09yOMlPur/v9427pqWW5LPdv+sfJ3k8yVvGXdNiS/JIkvNJfjzQtjrJU0le6LarFmOsZRf6jT7q4QLwuap6N3AbsLuBa37FvcDJcRcxQl8BvlVV7wLewzK/9iRrgU8Dk1V1M7M/ANk+3qqWxKPA1jlte4CjVbUJONod97bsQp8GH/VQVeeq6tlu/3fMBsHa8Va19JKsA+4EHhp3LaOQ5FrgA8DDAFX1h6r69ViLGo2VwFuTrASuZhne21NVPwB+Nad5G3Cg2z8A3L0YYy3H0F8LnB04nqaBAHxFkg3ALcCxMZcyCl8GPg/8acx1jMo7gBngq92S1kNJrhl3UUupqn4OfBE4A5wDflNV3xlvVSNzQ1Wdg9mJHXD9Yrzpcgz9y3rUw3KU5G3A14HPVNVvx13PUkpyF3C+qp4Zdy0jtBJ4L/BgVd0C/J5F+i//G1W3jr0N2AjcCFyT5GPjrerKthxDv8lHPSR5E7OB/1hVPTHuekbgduDDSX7G7BLeXyX52nhLWnLTwHRVvfK/uMPMfggsZx8CTlfVTFX9EXgCeP+YaxqVl5KsAei25xfjTZdj6Df3qIckYXad92RVfWnc9YxCVe2tqnVVtYHZv+PvVtWyngFW1S+As0ne2TVtAU6MsaRROAPcluTq7t/5Fpb5l9cDjgA7uv0dwJOL8aajfsrmkhvDox7eCG4HPg78KMkPu7YvVNU3x1eSlsingMe6Cc1PgU+MuZ4lVVXHkhwGnmX2V2rPsQwfyZDkceCDwHVJpoH7gPuBQ0l2Mvvhd8+ijOVjGCSpHctxeUeSdAmGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWrI/wHEH8Mbn4+gTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.hist(y_train,width=0.8,bins=range(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see: the classes are roughly balanced. This makes life easier. If the class distribution is severely skewed we need to take extra precautions. If the training data consists of 990 pictures of cats and only 10 pictures of dogs then prediction becomes too easy: don't even look at the pictures and simply answer 'cat'. Obviously, that procedure has not learnt how a cat picture looks like.\n",
    "\n",
    "## Train and evaluate logistic regression\n",
    "\n",
    "Let's start by training a logistic regression. Afterwards we evaluate it with the test set. Warning: this will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henning/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(solver='sag',max_iter=500)  \n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "train_err=zero_one_loss(y_train,logreg.predict(X_train))\n",
    "print('training error: {:.3}%'.format(train_err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay didn't converge, the training error looks good though. It took a while. \n",
    "\n",
    "Let's compute the test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 13.8%\n"
     ]
    }
   ],
   "source": [
    "test_err=zero_one_loss(y_test,logreg.predict(X_test))\n",
    "print('test error: {:.3}%'.format(test_err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, much worse! We see, a good training error does not guarantee a good test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Let's fit a decision tree. (We'll say later how a decision tree works.) A decision tree can fit a dataset extremely well. So hopefully, it'll be a good classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dectree=DecisionTreeClassifier()\n",
    "dectree.fit(X_train,y_train)\n",
    "train_err=zero_one_loss(y_train,dectree.predict(X_train))\n",
    "print('training error: {:.3}%'.format(train_err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, perfect classification! So, it's a good classifier, right? Let's try on the test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 19.1%\n"
     ]
    }
   ],
   "source": [
    "test_err=zero_one_loss(y_test,dectree.predict(X_test))\n",
    "print('test error: {:.3}%'.format(test_err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a really large error! Obviously, small training error does not *at all* indicate good performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "one_nn=KNeighborsClassifier(n_neighbors=1)\n",
    "one_nn.fit(X_train,y_train)\n",
    "train_err=zero_one_loss(y_train,one_nn.predict(X_train))\n",
    "print('training error: {:.3}%'.format(train_err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A zero training error is not at all surprising, if you think about it for a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 5.06%\n"
     ]
    }
   ],
   "source": [
    "test_err=zero_one_loss(y_test,one_nn.predict(X_test))\n",
    "print('test error: {:.3}%'.format(test_err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see: a much better test error than above!\n",
    "\n",
    "By the way, that we compare different classifiers with the help of the test error is quite typical. Based on these results we might then decide that nearest neighbour is the most suited algorithm for this task. (It's not! There are much better classifiers.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
