{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, _ = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As you point out, the experiments would be difficult. But we know\n",
      "enough about the physics of the situation to do some calculations.\n",
      "There are in fact three effects contributing to leaning the bike over\n",
      "to begin a turn.\n",
      "\n",
      "\t1. Gyro effect causing a torque which twists the bike over.\n",
      "\n",
      "\t2. Contact patch having shifted to one side, causing bike to fall over.\n",
      "\n",
      "\t3. Contact patch being accelerated to the side, causing a\n",
      "\ttorque which twists the bike over.\n",
      "\n",
      "Take an average bike/rider, average bike wheel, and at speeds of 5,\n",
      "15, and 50 mph (say) calculate how much twist of the bars would be\n",
      "needed to produce (say) 20 degrees of lean in (say) 2 seconds by each\n",
      "effect alone. My guess is that at slow speeds 2 is dominant, and at\n",
      "high speeds 3 is dominant, and at all speeds 1 contributes not far off\n",
      "bugger all, relatively speaking.\n",
      "\n",
      "By the way, a similar problem is this: how does a runner who wants to\n",
      "run round a corner get leaned into the corner fast? Is there a running\n",
      "group where we could start \"counter-footing\" arguments and have them\n",
      "all falling over as they tried to work out how they go round corners?\n"
     ]
    }
   ],
   "source": [
    "print(corpus[8567])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 101631\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "vector_data=vectorizer.fit_transform(corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite large, so let's do this again. This time we set max_df, the maximum frequency of words to be included in the dictionary. We set it to max_df=0.9, which means that words that occur in more than 90% of the documents are filtered out. We also set a minimum count, min_df=10, which has the effect that a word needs to appear in at least ten documents to be included in the dictionary. Finally, let's filter out stop words. \n",
    "\n",
    "A more professional approach would use a lemmatiser, for instance the lemmatiser of spaCy, but let's keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 10441\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(max_df=0.9, min_df=10,stop_words='english')\n",
    "vector_data=vectorizer.fit_transform(corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the Latent Dirichlet Allocation with ten topics (n_components=10). Because it's less memory intensive, we do online (ie minibatch) learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda=LatentDirichletAllocation(n_components=10,learning_method='online')\n",
    "lda.fit(vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the top words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: don know just like think time ve people good want\n",
      "Topic 1: key public use information government encryption law chip number security\n",
      "Topic 2: god people jesus believe does bible evidence say life true\n",
      "Topic 3: year game team games good play season hockey think players\n",
      "Topic 4: edu com new car drive price power hard used sale\n",
      "Topic 5: ax max g9v b8f a86 pl 145 1d9 34u 1t\n",
      "Topic 6: space nasa research center 1993 earth health university years water\n",
      "Topic 7: file use windows edu software available program using version files\n",
      "Topic 8: 00 10 25 11 15 17 16 12 14 20\n",
      "Topic 9: people said gun mr government israel armenian war state armenians\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "n_top_words=10\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    message=\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(\"Topic {}: \".format(i)+message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's obviously still a bit of garbage in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No. The issue is reducing crime, not guns. If gun control doesn't\n",
      "lower crime overall, then is doesn't address the issue.\n",
      "\n",
      "\n",
      "Does that matter if assaults with a baseball bat become much\n",
      "more common? Muggers using a gun rely primarily on the\n",
      "threat of the gun, and rarely shoot their victim. A mugger\n",
      "using a knife is much more likely to start by stabbing his victim \n",
      "in an effort incapacitate him. So, while a knif may not\n",
      "be as deadly as a gun, criminals are more likely to actually\n",
      "_use_ the knife (as opposed to threatening the victim with it.)\n",
      "It isn't at all clear that replacing the criminal's gun with a\n",
      "knife would reduce murders. Stabbings might just become more\n",
      "common. That's why it is important to look at the overall\n",
      "(not the with-gun) homicide rate. It avoids the issue of\n",
      "substitution, different criminal techinques of using different\n",
      "weapons, etc... and measures what we want to prevent: Murders.\n",
      "\n",
      "\n",
      "\"Face\"? Possibly. However, facing knife-welding attackers isn't\n",
      "too common: Stabbing without warning and by supprise is the\n",
      "usual tactic. Very few criminals shoot from cover: It attracts\n",
      "to much attention and they don't have a chance to go through your\n",
      "pockets. Overall, I'd much rather be threatened with a gun\n",
      "than actually stabbed with a knife.\n",
      "\n",
      "\n",
      "Actually, the exact same statement is true of guns: Training in\n",
      "unarmed self-defence will let you disarm an untrained gunman \n",
      "without much problem.\n",
      "\n",
      "You also ignore the criminal's reaction: The National Crime\n",
      "Survey clearly shows that criminals (unarmed, armed with a\n",
      "knife, gun or whatever) are unwilling to risk their lives\n",
      "in a confrontation. If faced with a serious threat, almost\n",
      "all prefer to leave and find an easier target. Therefore,\n",
      "using (or threatening to use, as is much more commonly the case)\n",
      "a weapon _is_ the best defence against an attacker, regardless\n",
      "of how he is armed. Knives, however, are much less effective\n",
      "than guns: Criminals don't consider knifes as a \"serious threat\"\n",
      "nearly as often as they do guns.\n"
     ]
    }
   ],
   "source": [
    "doc_num=1234\n",
    "print(corpus[doc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 19.51%\n",
      "Topic 1: 7.07%\n",
      "Topic 2: 7.91%\n",
      "Topic 3: 7.53%\n",
      "Topic 6: 4.73%\n",
      "Topic 9: 52.97%\n"
     ]
    }
   ],
   "source": [
    "topic_mix=lda.transform(vectorizer.transform([corpus[doc_num]]))[0]\n",
    "for i,proportion in enumerate(topic_mix):\n",
    "    if proportion > 0.005:\n",
    "        print(\"Topic {}: {:.2f}%\".format(i,proportion*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems about right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
