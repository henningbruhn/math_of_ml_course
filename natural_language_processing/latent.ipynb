{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "We try out scikit learn's LDA method and look at some best practices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(42)  # for better reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fetch a sample corpus that consists of a news group posts (basically, twitter with no character limit and no pictures). We hold back about 10% of documents for evaluation. We fix the number of topics relatively arbitrarily to 12. Moreover, we get ourselves an LDA object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# documents in corpus: 10183\n",
      "# documents held back: 1131\n"
     ]
    }
   ],
   "source": [
    "corpus, _ = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),return_X_y=True)\n",
    "random.shuffle(corpus)\n",
    "test_size=len(corpus)//10\n",
    "corpus,held_corpus=corpus[test_size:],corpus[:test_size]\n",
    "\n",
    "print(\"# documents in corpus: {}\".format(len(corpus)))\n",
    "print(\"# documents held back: {}\".format(len(held_corpus)))\n",
    "num_topics=12\n",
    "lda=LatentDirichletAllocation(n_components=num_topics,learning_method='online')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to look at some samples. (Here, we only look at one but please poke around a bit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What resources and services are available on Internet/BITNET which\n",
      "would be of interest to hospitals and other medical care providers?\n",
      "I'm interested in anything relelvant, including institutions and\n",
      "businesses of interest to the medical profession on Internet,\n",
      "special services such as online access to libraries or diagnostic\n",
      "information, etc. etc.\n"
     ]
    }
   ],
   "source": [
    "print(corpus[8567])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt: Run scikit learn's LDA out of the box\n",
    "\n",
    "We first try what happens when we use LDA without any fancy processing and so on. In the first step, we need to turn each text document into a vector. This is done by <code>CountVectorizer</code> which sets up a bag-of-words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 91960\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "vector_data=vectorizer.fit_transform(corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary seems rather large for only about 10000 documents. Next, we fit the LDA model on the vectorised data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 107.2s\n"
     ]
    }
   ],
   "source": [
    "def run_fit(data):\n",
    "    start=time.time()\n",
    "    lda.fit(data)\n",
    "    end=time.time()\n",
    "    print(\"Fitting took {:2.1f}s\".format(end-start))\n",
    "\n",
    "run_fit(vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the topic-term probabilities with <code>lda.components_</code>. However, this is just a list of probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08334693, 0.08333764, 1.94665496, ..., 0.08333334, 0.08333334,\n",
       "       0.08333334])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand what the topics actually means we pick out the terms with largest probability for each topic. Hopefully, we'll see then that the topics really capture a natural concept that is present in the corpus. \n",
    "\n",
    "How prevalent a topic is in the corpus is obviously of interest, too. We compute that by summing the topic weight over all documents in the corpus. We then normalise to get a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 10: 41.1% -> the to and is it of that you for in this have be with on\n",
      "Topic  7: 22.9% -> the of to and that is in it you not are as be this for\n",
      "Topic 11: 19.1% -> the and to in of that was he they on it for at you but\n",
      "Topic  5:  8.5% -> the and of for to in is on file from edu by or are program\n",
      "Topic  1:  2.2% -> 00 10 25 15 20 16 11 12 14 13 18 17 55 30 24\n",
      "Topic  4:  1.7% -> of university 1993 national by health april research center states united and in medical information\n",
      "Topic  8:  1.1% -> gm at de coli candida infections infection mydisplay ci son cd moncton rochester een loser\n",
      "Topic  0:  0.9% -> db entry output entries rules oname mov contest fprintf excellent cs year check_io uuencode ___\n",
      "Topic  6:  0.7% -> pl 1t 3t bh qq 0d m3 mq sl ah 34 d9 5u 7u m_\n",
      "Topic  2:  0.7% -> maine en op_cols op_rows da kotl heh turk ul ob iv reuss salonica bedouin ermeni\n",
      "Topic  9:  0.6% -> cx w7 c_ hz uw t7 ck chz lk w1 17 mv k8 sk a7\n",
      "Topic  3:  0.5% -> ax max g9v b8f a86 145 1d9 0t bhj giz wm 2di 2tm 75u 34u\n"
     ]
    }
   ],
   "source": [
    "def show_topic_stats(lda,corpus,num_top_words=15):\n",
    "    topic_mixes=lda.transform(vectorizer.transform(corpus))\n",
    "    total_topic_weights=topic_mixes.sum(axis=0)\n",
    "    rel_topic_weights=total_topic_weights/sum(total_topic_weights)    \n",
    "    topics_by_weight=sorted([(topic,topic_weight) for topic,topic_weight in enumerate(rel_topic_weights)],key=itemgetter(1),reverse=True)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic,topic_weight in topics_by_weight:\n",
    "        topic_dist=lda.components_[topic]\n",
    "        top_terms=topic_dist.argsort()[:-num_top_words - 1:-1]\n",
    "        message=\" \".join([feature_names[i] for i in top_terms])\n",
    "        print(\"Topic {:2}: {:4.1f}% -> \".format(topic,topic_weight*100)+message)\n",
    "\n",
    "show_topic_stats(lda,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously garbage. In particular we see many common short words, such as \"the\", \"to\", \"and\", and a good number of terms that are just rubbish (\"g9v\", \"__\" etc). \n",
    "\n",
    "While it is clear that these topics are not useful, we still try to evaluate them numerically. This is not easy to do well. Here, we look at two measures: the log likelihood of a whole corpus and the [perplexity](https://en.wikipedia.org/wiki/Perplexity) per word. Both are evaluated on the held back corpus. Larger values are better for the log likelihood; smaller values are better for perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood of test corpus : -1927517.24\n",
      "word perplexity of test corpus:  0.0279\n"
     ]
    }
   ],
   "source": [
    "def print_validation(test_corpus,old_scores=None):\n",
    "    test_data=vectorizer.transform(held_corpus)\n",
    "    score=lda.score(test_data)\n",
    "    num_words=sum([len(doc.split(\" \")) for doc in test_corpus])\n",
    "    perplexity=lda.perplexity(test_data)/num_words\n",
    "    old_like_str=\"\"\n",
    "    old_perp_str=\"\"\n",
    "    if old_scores is not None:\n",
    "        old_like,old_perp=old_scores\n",
    "        old_like_str=\" (was: {:.2f})\".format(old_like)\n",
    "        old_perp_str=\" (was: {:.4f})\".format(old_perp)\n",
    "    print(\"log likelihood of test corpus : {:.2f}\".format(score)+old_like_str)\n",
    "    print(\"word perplexity of test corpus:  {:.4f}\".format(perplexity)+old_perp_str)\n",
    "    return score,perplexity\n",
    "    \n",
    "first_val=print_validation(held_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to interpret these numbers at this moment. They will become useful below, when we can compare different models. (This is also the reason why the code is a little bit more complicated then we'd need right now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attempt: Filter very frequent words\n",
    "\n",
    "Very frequent words do not tell us anything about the topic of a document. The word \"the\" will occur in almost every document but will stil be accomodated by the LDA model. So, it's better to filter out such words. \n",
    "\n",
    "First let us have a look at the terms with largest document frequency, ie, at the words that appear in the largest number of documents. (Note, that we do not count here how often the word appears in total, but in how many document it appears.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words occuring most frequently in different documents:\n",
      " the   : 8357\n",
      " to    : 7533\n",
      " a     : 7342\n",
      " of    : 6816\n",
      " and   : 6774\n",
      " I     : 6313\n",
      " in    : 6110\n",
      " is    : 6020\n",
      " that  : 5615\n",
      " for   : 5439\n",
      " it    : 4726\n",
      " have  : 4574\n",
      " on    : 4411\n",
      " be    : 4338\n",
      " with  : 4261\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def document_frequency(corpus):\n",
    "    doc_freq=defaultdict(int)\n",
    "    for document in corpus:\n",
    "        for token in set(document.split()):\n",
    "            doc_freq[token]+=1\n",
    "    return doc_freq\n",
    "\n",
    "def print_top_df_words(corpus,num=15):\n",
    "    df=document_frequency(corpus)\n",
    "    top_df_words=sorted([(key,count) for key,count in df.items()],key=itemgetter(1),reverse=True)\n",
    "    print(\"words occuring most frequently in different documents:\")\n",
    "    for key,count in top_df_words[:num]:\n",
    "        print(\" {:6}: {}\".format(key,count))\n",
    "\n",
    "print_top_df_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is not a surprise and clearly none of these terms helps to understand what a given document is about. We filter them out. <code>CountVectorizer</code> can do that quite conveniently. We only have to pass <code>max_df=0.1</code> and then it will suppress all terms that appear in more than 10% of the documents. We also set <code>min_df=10</code> to filter out all those terms that appear in only up to 10 documents. (A fractional value, e.g. <code>min_df=0.01</code>, will filter out all terms that appear in fewer than 1% of the documents.)\n",
    "\n",
    "By the way, it seems it's good to be aggressive when filtering out words that are too common. Smaller fractions than 10% document frequency (perhaps 5%?) may lead to even better results. A large maximum document frequency (ie, 50%) will cull only a few extremely common words; see the list above. What is best here likely depends on the corpus and the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 9736\n",
      "Fitting took 43.7s\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(max_df=0.1, min_df=10)\n",
    "vector_data=vectorizer.fit_transform(corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))\n",
    "run_fit(vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the vocabulary is much smaller than previously, which also results in much faster fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  3: 22.2% -> re better really sure since something might problem work ll doesn probably still thing point\n",
      "Topic  0: 13.3% -> god jesus believe us him our christian bible true did question must life church law\n",
      "Topic  1: 11.6% -> file edu windows program com files window ftp available version db graphics server image using\n",
      "Topic  6: 10.5% -> she car him her said back off went go didn down got over us left\n",
      "Topic 11:  9.3% -> drive card system dos disk scsi hard video pc memory mac drives speed monitor apple\n",
      "Topic  4:  8.4% -> gun right israel us against children israeli rights anti our police state said human guns\n",
      "Topic 10:  6.6% -> space university edu information 00 nasa 1993 research data 10 center available 20 list contact\n",
      "Topic  2:  6.0% -> 10 00 game team 25 15 17 11 12 16 20 games 14 13 year\n",
      "Topic  8:  5.1% -> mr president government states state national our american bill united public health congress groups years\n",
      "Topic  5:  4.1% -> key encryption chip keys clipper security output number system privacy information public government law data\n",
      "Topic  7:  2.0% -> armenian armenians turkish turkey jews russian war armenia turks soviet greek genocide history azerbaijan government\n",
      "Topic  9:  0.9% -> ax max g9v b8f a86 pl 145 1d9 0t 1t bhj 34u 3t cx wm\n"
     ]
    }
   ],
   "source": [
    "show_topic_stats(lda,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While still not good, we see that now at least some of the topics make sense. Still there are some garbage terms that pollute the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood of test corpus : -897250.40 (was: -1927517.24)\n",
      "word perplexity of test corpus:  0.0146 (was: 0.0279)\n"
     ]
    }
   ],
   "source": [
    "second_val=print_validation(held_corpus,old_scores=first_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, log likelihood and word perplexity have improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third attempt: More professional preprocessing\n",
    "\n",
    "We use the <code>gensim</code> library to improve our preprocessing. See https://radimrehurek.com/gensim/ for the documentation.\n",
    "\n",
    "Let us take a sample and let us see how <code>gensim</code> transforms it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Since running any GUI over a network is going to slow it down by a\n",
      "fair amount, I expect Windows NT will be multiuser only in the sense\n",
      "of sharing filesystems. Someone will likely write a telnetd for it so\n",
      "one could run character-based apps, but graphics-based apps will have\n",
      "to be shared by running the executables on the local CPU. This is how\n",
      "things are shaping up everywhere: client-server architectures are\n",
      "taking over from the old cpu-terminal setups. \n",
      "\n",
      "Note that the NeXT does this: you can always telnet into a NeXT and\n",
      "run character-based apps but you can't run the GUI. (Yeah, I know\n",
      "about X-Windows, just haven't been too impressed by it...)..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- \n"
     ]
    }
   ],
   "source": [
    "sample_doc=corpus[5678]\n",
    "print(sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>gensim</code> has a number of preprocessing routines. We use <code>gensim.parsing.preprocessing.preprocess_string</code> that takes as argument a number of filters. We lower case everything, we strip punctuation marks (,.1?_ etc), we throw away numbers and other special characters, we remove very short words (up to two characters) and stop words. Let's look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running gui network going slow fair expect windows multiuser sense sharing filesystems likely write telnetd run character based apps graphics based apps shared running executables local cpu things shaping client server architectures taking old cpu terminal setups note telnet run character based apps run gui yeah know windows haven impressed\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import *\n",
    "filters=[lambda x: x.lower(),strip_tags,strip_punctuation,strip_multiple_whitespaces,strip_numeric,remove_stopwords,strip_short]\n",
    "\n",
    "processed=gensim.parsing.preprocessing.preprocess_string(sample_doc,filters=filters)\n",
    "print(\" \".join(processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the sample document has become much shorter and many non-interesting terms are gone. \n",
    "\n",
    "Let us look at the stop words that were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can cannot cant co computer con could couldnt cry de describe detail did didn do does doesn doing don done down due during each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fifty fill find fire first five for former formerly forty found four from front full further get give go had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie if in inc indeed interest into is it its itself just keep kg km last latter latterly least less ltd made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put quite rather re really regarding same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under unless until up upon us used using various very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sorted(gensim.parsing.preprocessing.STOPWORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we process the whole corpus (and also the held back corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters=[lambda x: x.lower(),strip_tags,strip_punctuation,strip_multiple_whitespaces,strip_numeric,remove_stopwords,strip_short]\n",
    "\n",
    "def preprocess_corpus(corpus,filters=filters):\n",
    "    preprocessed_corpus=[]\n",
    "    for document in corpus:\n",
    "        preprocessed=gensim.parsing.preprocessing.preprocess_string(document,filters=filters)\n",
    "        preprocessed_document=\" \".join(preprocessed)\n",
    "        preprocessed_corpus.append(preprocessed_document)\n",
    "    return preprocessed_corpus\n",
    "\n",
    "preprocessed_corpus=preprocess_corpus(corpus)\n",
    "preprocessed_held_corpus=preprocess_corpus(held_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the bag-of-words model and we fit the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 8520\n",
      "Fitting took 38.2s\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(max_df=0.1, min_df=10)\n",
    "vector_data=vectorizer.fit_transform(preprocessed_corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))\n",
    "run_fit(vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, vocabulary and running time have improved again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  9: 19.9% -> going better right said got little things years lot look come let thing maybe sure\n",
      "Topic  0: 12.4% -> problem help mail looking work email windows read info post advance sound send sure appreciated\n",
      "Topic 11: 11.1% -> state government right gun law israel jews rights states case fact question israeli issue point\n",
      "Topic  1:  8.9% -> max file edu available window program ftp files image graphics version server windows software code\n",
      "Topic 10:  8.2% -> game team year edu play games season win hockey players league san period com teams\n",
      "Topic  3:  7.9% -> drive card dos disk scsi hard memory video mac drives monitor bit apple board controller\n",
      "Topic  5:  7.0% -> god jesus believe bible christian church life christians faith christ religion true man book truth\n",
      "Topic  7:  6.1% -> space president information research nasa national university program data center april technology earth science available\n",
      "Topic  2:  5.7% -> car power bike cars engine price sale goal shipping sell excellent radio condition miles gas\n",
      "Topic  8:  4.8% -> key encryption chip keys clipper security public privacy com information phone government number algorithm message\n",
      "Topic  4:  4.7% -> health medical food father son children disease pain doctor patients study holy death spirit child\n",
      "Topic  6:  3.6% -> armenian armenians war turkish said russian turkey killed water women ground soviet armenia land soldiers\n"
     ]
    }
   ],
   "source": [
    "show_topic_stats(lda,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! A lot of the garbage is gone and many of the topics are recognisable. There are still some issues, though. We may see singular and plural form of some nouns (\"car\" and \"cars\") -- surely, if some document talks about \"cars\" and some other about a \"car\" then both documents are about \"motor vehicles\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood of test corpus : -682352.84 (was: -897250.40)\n",
      "word perplexity of test corpus:  0.0897 (was: 0.0146)\n"
     ]
    }
   ],
   "source": [
    "third_val=print_validation(preprocessed_held_corpus,old_scores=second_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again log likelihood has improved. Word perplexity has not though. This is most likely because many common words are now suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth attempt: Stemming\n",
    "\n",
    "\"Car\" and \"cars\" designate the same topic, \"run\" and \"running\" too. *Stemming* is a technique that reduces words to their root, so that both \"runs\" and \"running\" become \"run\". We use here a simple stemming algorithm, *Porter stemming*. Other libraries implement more sophisticated stemming methods that try to identify whether the term is a noun, a verb and so on to do a better stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i run run she run lexicograph lexicograph'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "document=\"I run running she runs lexicographic lexicographically\"\n",
    "porter_stemmer.stem_sentence(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run gui network go slow fair expect window multius sens share filesystem like write telnetd run charact base app graphic base app share run execut local cpu thing shape client server architectur take old cpu termin setup note telnet run charact base app run gui yeah know window haven impress\n"
     ]
    }
   ],
   "source": [
    "filters=[lambda x: x.lower(),strip_tags,strip_punctuation,strip_multiple_whitespaces,strip_numeric,remove_stopwords,strip_short,stem_text]\n",
    "\n",
    "processed=gensim.parsing.preprocessing.preprocess_string(sample_doc,filters=filters)\n",
    "print(\" \".join(processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately identify a problem: Some of the terms are no longer recognisable as English words (\"multius\", \"charact\"). To be able to go back from the stemmed terms to the original terms we compute a dictionary <code>lemma_mapping</code>. (My excuses: the code is perhaps a bit more complicated than necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed term \"multius\" came from \"multiuser\"\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer=PorterStemmer()\n",
    "filters=[lambda x: x.lower(),strip_tags,strip_punctuation,strip_multiple_whitespaces,strip_numeric,remove_stopwords,strip_short]\n",
    "\n",
    "def preprocess_document(document,lemma_mapping,filters=filters):\n",
    "    result = []\n",
    "    processed=gensim.parsing.preprocessing.preprocess_string(document,filters=filters)\n",
    "    for token in processed:\n",
    "        lemma = porter_stemmer.stem(token)\n",
    "        result.append(lemma)\n",
    "        # map lemma to token\n",
    "        # ...if lemma not yet in dictionary\n",
    "        # ...or with small probability (thus, more likely that lemma is mapped to a common term)\n",
    "        if (lemma not in lemma_mapping.keys()) or (random.random()<0.05):\n",
    "            lemma_mapping[lemma]=token\n",
    "    return result\n",
    "\n",
    "def preprocess_corpus_stemming(corpus,filters=filters):\n",
    "    lemma_mapping={}\n",
    "    result=[]\n",
    "    for document in corpus:\n",
    "        result.append(\" \".join(preprocess_document(document,lemma_mapping,filters=filters)))\n",
    "    return result,lemma_mapping\n",
    "\n",
    "preprocessed_corpus,lemma_mapping=preprocess_corpus_stemming(corpus)\n",
    "preprocessed_held_corpus,_=preprocess_corpus_stemming(held_corpus)\n",
    "lemma=\"multius\"\n",
    "print('The stemmed term \"{}\" came from \"{}\"'.format(lemma,lemma_mapping[lemma]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful, we can do perform stemming but also recover the original term. (Or rather some common original term.) We vectorise and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 6207\n",
      "Fitting took 37.1s\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(max_df=0.1, min_df=10)\n",
    "vector_data=vectorizer.fit_transform(preprocessed_corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))\n",
    "run_fit(vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the size of the vocabulary has decreased further due to stemming. Because of stemming, we need to adapt the method <code>show_topic_stats</code> to take advantage of our inverse stemming-map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  5: 18.5% -> going car day started let said happen surely lots tells says ask got gets went\n",
      "Topic  6: 12.3% -> god believe christian jesus means questions existing point word reasonably claim argument true bible personally\n",
      "Topic  2: 11.2% -> max file windows programs version running dos images set display server graphics available colors software\n",
      "Topic 11:  8.9% -> drive cards guns disk controlled driver scsi price hard monitor board video mac speed sale\n",
      "Topic  4:  7.8% -> games play team players season win scoring league hockey goal period running second fan point\n",
      "Topic  3:  7.4% -> space development data research nasa programs orbiter centers science designation images including model information universal\n",
      "Topic  8:  7.1% -> bikes rides dod appears little left got black cause motorcycles turned flame picture better man\n",
      "Topic  7:  6.4% -> state government president national american law israel public united weapons country israeli health report politic\n",
      "Topic 10:  5.8% -> armenian kill said jews turkish history children women muslim jewish greeks sons disease turkey war\n",
      "Topic  1:  5.4% -> edu com mail lists send email address requesting contacting info internet ftp universal pub site\n",
      "Topic  0:  4.6% -> keys encryption chip secure information public clipper number phone message bit author enforcement law technology\n",
      "Topic  9:  4.6% -> power wired ground water current connect autos circuit installed switch replacement light supply require battery\n"
     ]
    }
   ],
   "source": [
    "def show_topic_stats(lda,corpus,lemma_mapping,num_top_words=15):\n",
    "    topic_mixes=lda.transform(vectorizer.transform(corpus))\n",
    "    total_topic_weights=topic_mixes.sum(axis=0)\n",
    "    rel_topic_weights=total_topic_weights/sum(total_topic_weights)    \n",
    "    topics_by_weight=sorted([(topic,topic_weight) for topic,topic_weight in enumerate(rel_topic_weights)],key=itemgetter(1),reverse=True)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic,topic_weight in topics_by_weight:\n",
    "        topic_dist=lda.components_[topic]\n",
    "        top_terms=topic_dist.argsort()[:-num_top_words - 1:-1]\n",
    "        message=\" \".join([lemma_mapping[feature_names[i]] for i in top_terms])\n",
    "        print(\"Topic {:2}: {:4.1f}% -> \".format(topic,topic_weight*100)+message)\n",
    " \n",
    "show_topic_stats(lda,preprocessed_corpus,lemma_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics appear to make more and more sense. There is clearly a tech topic, there is a religion topic, there is a topic about guns and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood of test corpus : -621667.91 (was: -682352.84)\n",
      "word perplexity of test corpus:  0.0825 (was: 0.0897)\n"
     ]
    }
   ],
   "source": [
    "fourth_val=print_validation(preprocessed_held_corpus,old_scores=third_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood and word perplexity have improved somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth attempt: n-grams\n",
    "\n",
    "How can we improve the model further? One issue with our approach: We implicitly assume that spaces separate the terms. This is true in a sentence such as \"I am eating cake\". In the sentence \"Ulm University is awesome\", however, the terms \"Ulm\" and \"University\" became separate, which seems wrong. We do not talk about Ulm and also not about universities in general but about a specific university, namely \"Ulm university\". To identify such terms we look at all 2-grams, pairs of words that appear consecutively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my cat', 'cat is', 'is eating', 'eating cake']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twograms(sentence):\n",
    "    word_list=sentence.split()\n",
    "    if len(word_list)<=1:\n",
    "        return word_list\n",
    "    result=[]\n",
    "    for i,word in enumerate(word_list[1:]):\n",
    "        result.append(word_list[i]+\" \"+word)\n",
    "    return result\n",
    "\n",
    "twograms(\"my cat is eating cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we identify the most frequent 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of the', 10394),\n",
       " ('in the', 7120),\n",
       " ('to the', 4042),\n",
       " ('on the', 3763),\n",
       " ('to be', 2911),\n",
       " ('it is', 2887),\n",
       " ('for the', 2729),\n",
       " ('that the', 2641),\n",
       " ('is a', 2497),\n",
       " ('and the', 2433),\n",
       " ('i have', 2225),\n",
       " (\"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>' max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\",\n",
       "  2203),\n",
       " ('if you', 2126),\n",
       " ('with the', 2030),\n",
       " ('this is', 1845)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_occurences(twograms_list):\n",
    "    count=defaultdict(int)\n",
    "    for twogram in twograms_list:\n",
    "        count[twogram]+=1\n",
    "    return count\n",
    "\n",
    "def top_twograms(corpus,num=15):\n",
    "    twograms_list=[]\n",
    "    for document in corpus:\n",
    "        twograms_list.extend(twograms(document.lower()))\n",
    "    count_dict=count_occurences(twograms_list)\n",
    "    top=sorted([(key,count) for key,count in count_dict.items()],key=itemgetter(1),reverse=True)\n",
    "    return top[:num]\n",
    "\n",
    "top_twograms(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that did not work. Obviously, we first have to filter out the garbage and *then* look for frequent 2-grams. Note that we don't do stemming yet. This is mostly, so that we can still recognise the terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('max max', 2838),\n",
       " ('united states', 289),\n",
       " ('new york', 264),\n",
       " ('thanks advance', 222),\n",
       " ('years ago', 210),\n",
       " ('law enforcement', 207),\n",
       " ('anonymous ftp', 194),\n",
       " ('mit edu', 184),\n",
       " ('clipper chip', 172),\n",
       " ('los angeles', 148),\n",
       " ('hard disk', 141),\n",
       " ('let know', 136),\n",
       " ('looks like', 133),\n",
       " ('power play', 131),\n",
       " ('hard drive', 129),\n",
       " ('edu pub', 128),\n",
       " ('public key', 118),\n",
       " ('nasa gov', 114),\n",
       " ('bhj bhj', 114),\n",
       " ('year old', 111)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_corpus=preprocess_corpus(corpus)\n",
    "preprocessed_held_corpus=preprocess_corpus(held_corpus)\n",
    "top_twos=top_twograms(preprocessed_corpus,num=20)\n",
    "top_twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. \"United States\" and \"New York\" are certainly terms we want to keep. (Instead of \"United\" and \"States\".) \n",
    "\n",
    "Let's set up a function that turns a 2-gram \"United States\" into \"United_States\", so that when we split by spaces, the 2-grams stay together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my cat is moving from new_york to los_angeles and back to new_york',\n",
       " 'my new cat is born in york']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mark_twograms(corpus,twogram_list):\n",
    "    processed_corpus=[]\n",
    "    for document in corpus:\n",
    "        document=document.lower()\n",
    "        for twogram in twograms(document):\n",
    "            if twogram in twogram_list:\n",
    "                first_word,second_word=twogram.split()\n",
    "                document=document.replace(twogram,first_word+\"_\"+second_word)\n",
    "        processed_corpus.append(document)\n",
    "    return processed_corpus\n",
    "\n",
    "### let's test this\n",
    "toy_corpus=[\"my cat is moving from new york to los angeles and back to new york\",\"my new cat is born in york\"]\n",
    "mark_twograms(toy_corpus,[\"new york\",\"los angeles\",\"hard drive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we mark the most frequent 2-grams and then we perform stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twos=[twogram for twogram,_ in top_twos]\n",
    "preprocessed_corpus=mark_twograms(preprocessed_corpus,top_twos)\n",
    "preprocessed_held_corpus=mark_twograms(preprocessed_held_corpus,top_twos)\n",
    "preprocessed_corpus,lemma_mapping=preprocess_corpus_stemming(preprocessed_corpus,filters=[])  # only do stemming\n",
    "preprocessed_held_corpus,_=preprocess_corpus_stemming(preprocessed_held_corpus,filters=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, vectorisation and fitting. Note that the size of the vocabulary will go up a bit due to the 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 6224\n",
      "Fitting took 37.9s\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(max_df=0.1, min_df=10)\n",
    "vector_data=vectorizer.fit_transform(preprocessed_corpus)\n",
    "print(\"size of vocabulary: {}\".format(len(vectorizer.vocabulary_.keys())))\n",
    "run_fit(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  2: 12.1% -> key bit card encryption chips number data prices disk phone mac monitor control include devices\n",
      "Topic 11: 11.4% -> questions point different book case discussion general example reason mean person caused effect group idea\n",
      "Topic  7: 11.4% -> drives help tell got going read gets happen sure maybe heard started scsi set lot\n",
      "Topic  8: 10.8% -> filed windows program images available version running dos server include set applications lists graphics users\n",
      "Topic  3:  9.8% -> god christian believe jesus said jews lived israel religion bible word church says life mean\n",
      "Topic  4:  8.9% -> team games playing players win season goal hockey league city said pick flame san great\n",
      "Topic  6:  8.3% -> car engine power bike ground wires water rides light oil miles tired dod braking auto\n",
      "Topic  1:  8.0% -> government gun states law president american publication countries going issues weapon crime court person control\n",
      "Topic 10:  5.8% -> games max running went hit got second left reds scored guy lines blue came pitch\n",
      "Topic  9:  5.4% -> space center research university orbiting health medical nasa development program diseases launch food missions reported\n",
      "Topic  5:  4.5% -> edu com mail email address card pts bios internet van apr simm net bus send\n",
      "Topic  0:  3.5% -> armenians turkish war kill attacking russians privacy government turkey nazi germans azerbaijani clipper turks armenia\n"
     ]
    }
   ],
   "source": [
    "show_topic_stats(lda,preprocessed_corpus,lemma_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics don't look too different and no 2-gram is among the top words. Here I had more hopes. \n",
    "\n",
    "I should point out, though, that identifying phrases that belong together can be done a lot better. By restricting ourselves to 2-grams, for instance, we will never keep \"United States of America\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood of test corpus : -622211.22 (was: -621667.91)\n",
      "word perplexity of test corpus:  0.0842 (was: 0.0825)\n"
     ]
    }
   ],
   "source": [
    "fifth_val=print_validation(preprocessed_held_corpus,old_scores=fourth_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not much change here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Let's look at some of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "How about Kevin Hatcher? Scored roughly 35 goals, plays 30 minutes a game.\n",
      "\n",
      "\n",
      "That's really sad when two second-rate goalies (Barasso and Belfour)\n",
      "are the main contenders for the Vezina. Call me crazy, but how about\n",
      "Tommy Soderstrom - five shutouts for a 6th place team that doesn't\n",
      "really play defense. It's really unfortunate that the better goalies\n",
      "in the league (McLean, Essensa, Vernon) had unspectacular years. BTW,\n",
      "if you are going to award the Norris on the basis of the last 30 days,\n",
      "why not give the Vezina to Moog? He has been the best goalie over the\n",
      "past month.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arbour or King. B...\n",
      "\n",
      "Topic  4: 68.87%\n",
      "Topic 10: 20.07%\n",
      "Topic  7: 5.15%\n",
      "Topic  6: 4.68%\n"
     ]
    }
   ],
   "source": [
    "def show_doc_and_topic(doc_num,snippet_length=600):\n",
    "    if len(corpus[doc_num])>snippet_length:\n",
    "        print(corpus[doc_num][:snippet_length]+\"...\\n\")\n",
    "    else:\n",
    "        print(corpus[doc_num]+\"\\n\")\n",
    "    topic_mix=lda.transform(vectorizer.transform([preprocessed_corpus[doc_num]]))[0]\n",
    "    rel_topics=sorted([(i,proportion) for i,proportion in enumerate(topic_mix) if proportion>0.005],key=itemgetter(1),reverse=True)\n",
    "    for topic,proportion in rel_topics:\n",
    "        print(\"Topic {:2}: {:.2f}%\".format(topic,proportion*100))        \n",
    "\n",
    "show_doc_and_topic(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In <1993Apr15.143320.8618@desire.wright.edu>, demon@desire.wright.edu sez:\n",
      "\n",
      "There's this minor thing called \"interest of finality/repose\".  What\n",
      "it means is that parties aren't dragged into court over and over again\n",
      "because the losing side \"discovers\" some \"new\" evidence.  I don't know\n",
      "about you, Brett, but I suspect GM had the resources to find just\n",
      "about as many expert and fact witnesses as it wanted before the trial\n",
      "started.  Letting them re-open the case now is practically an\n",
      "invitation to every civil litigant on earth to keep an ace in the hole\n",
      "in case the verdict goes against him.\n",
      "\n",
      "BTW, ...\n",
      "\n",
      "Topic  1: 41.95%\n",
      "Topic 11: 24.84%\n",
      "Topic  3: 10.88%\n",
      "Topic  4: 7.88%\n",
      "Topic  0: 6.24%\n",
      "Topic  5: 5.59%\n",
      "Topic 10: 2.10%\n"
     ]
    }
   ],
   "source": [
    "show_doc_and_topic(5364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Something to bear in mind is what the V in VLB stands for!\n",
      "\n",
      "V for Video - the origional intention of the bus was to speed up\n",
      "the bus so that large memory to memory transfers would be faster.\n",
      "This is espically useful in transfering data from main memory to\n",
      "video memory.\n",
      "\n",
      "Since there are usually 3 VLB slots card makers have been making \n",
      "cards to fit in the other two. \n",
      "\n",
      "How about an VLB ethernet card? Move the data into the card at\n",
      "130 odd MB/s and then wait for it to tickle onto the net at\n",
      "just over 1Mb/s.\n",
      "\n",
      "[ Do do however free the local bus for other cards ]\n",
      "\n",
      "Some times you need fast busses an...\n",
      "\n",
      "Topic  2: 47.23%\n",
      "Topic  5: 27.74%\n",
      "Topic  7: 18.00%\n",
      "Topic  1: 5.52%\n"
     ]
    }
   ],
   "source": [
    "show_doc_and_topic(7654)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "\tHELP!!! please\n",
      "\t\tI am a student of turbo c++ and graphics programming\n",
      "\tand I am having some problems finding algorithms and code\n",
      "\tto teach me how to do some stuff..\n",
      "\n",
      "\t1) Where is there a book or code that will teach me how\n",
      "\tto read and write pcx,dbf,and gif files?\n",
      "\n",
      "\t2) How do I access the extra ram on my paradise video board\n",
      "\tso I can do paging in the higher vga modes ie: 320x200x256\n",
      "\t800x600x256\n",
      "\t3) anybody got a line on a good book to help answer these question?\n",
      "\n",
      "Thanks very much !\n",
      "\n",
      "send reply's to : Palm@snycanva.bitnet\n",
      "\n",
      "Topic  8: 30.12%\n",
      "Topic  2: 28.63%\n",
      "Topic 11: 21.97%\n",
      "Topic  7: 15.07%\n",
      "Topic  1: 2.82%\n"
     ]
    }
   ],
   "source": [
    "show_doc_and_topic(8737)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pick out some documents that score highly for a chosen topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has anybody generated an X server for Windows NT?  If so, are you willing\n",
      "to share your config file and other tricks necessary to make it work?\n",
      "\n",
      "Thanks for any information.\n",
      "\n",
      "Topic  8: 92.36%\n",
      "Topic  1: 0.69%\n",
      "Topic  7: 0.69%\n",
      "Topic 11: 0.69%\n",
      "Topic  2: 0.69%\n",
      "Topic  3: 0.69%\n",
      "Topic  0: 0.69%\n",
      "Topic  6: 0.69%\n",
      "Topic  4: 0.69%\n",
      "Topic 10: 0.69%\n",
      "Topic  9: 0.69%\n",
      "Topic  5: 0.69%\n"
     ]
    }
   ],
   "source": [
    "topic_mixes=lda.transform(vectorizer.transform(preprocessed_corpus))\n",
    "\n",
    "def show_top_doc_for_topic(topic_num):\n",
    "    for doc_num,topic_mix in enumerate(topic_mixes):\n",
    "        if topic_mix[topic_num]>0.7:\n",
    "            break\n",
    "    show_doc_and_topic(doc_num)\n",
    "    \n",
    "show_top_doc_for_topic(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actually, fired-coach George Kingston was a third of the GM\n",
      "triumvirate.  Now that the trio is now duo (Dean Lombardi and Chuck\n",
      "Grillo), the Sharks are already on their 3rd \"office of the GM\". And a\n",
      "4th is likely to happen before September; they'll either add the new\n",
      "coach to the OofGM, or name a single GM. So your wager should be\n",
      "amended to read that Sharks are likely to have their 5th GM before the\n",
      "Panther's get their 2nd. Can't wait to see how the next season's NHL\n",
      "Guide and Record Book lists the GM history of the Sharks.\n",
      "\n",
      "Given the depth of next year's draft, the expansion draft rules, an...\n",
      "\n",
      "Topic  4: 91.40%\n",
      "Topic  5: 2.61%\n",
      "Topic  1: 2.51%\n",
      "Topic 11: 2.30%\n"
     ]
    }
   ],
   "source": [
    "show_top_doc_for_topic(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wondering\n",
      "                                                      -------------\n",
      "\n",
      "Do you mean Juan Berenguer?  He was traded for Mark Davis in the middle\n",
      "of last season.  Exchanged one stiff for another, as Berenguer hadn't\n",
      "come back from his injury in 91.  I think he's retired now.\n",
      "\n",
      "Anyhow, as middle relief, Marvin ain't that bad.  He at least can\n",
      "pitch a couple of innings or do mop-up work.  I don't know much\n",
      "about McMichael (was he the Mexican League guy?), but\n",
      "everybody else in the pen is a 1 inning man, except maybe\n",
      "Mercker.\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Eric Rou...\n",
      "\n",
      "Topic 10: 70.83%\n",
      "Topic  4: 13.19%\n",
      "Topic  7: 7.63%\n",
      "Topic  5: 6.50%\n"
     ]
    }
   ],
   "source": [
    "show_top_doc_for_topic(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 09/09/22"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
