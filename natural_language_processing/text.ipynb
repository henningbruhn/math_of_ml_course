{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing in natural language processing\n",
    "\n",
    "A common NLP library in python is *spacy*. You might need to install it first via <code>pip install spacy</code>, or if you're running this in google colab, via <code>!pip install spacy</code>. First we import *spacy* and load the corpus, ie, the text database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next line if you need to install spacy\n",
    "#!pip install spacy\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm') # a small corpus of English text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "First step: split the text sample into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'bears',\n",
       " 'and',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'stand',\n",
       " 'their',\n",
       " 'smell',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"I'm afraid of bears and can't stand their smell.\")\n",
    "[w.text for w in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might seem weird: \"'m\" and \"n't\" have each become a token. That is because they do represent a part of the speech that is really separate from the word they're in. We see in the next step why treating them separately is a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "Lemmatisation transforms a token into its base form. \"am\" or \"'m\" becomes \"be\", \"bears\" becomes \"bear\" and so on. \"I/you/she\" etc becomes \"-PRON-\", a marker for \"pronoun\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'bear',\n",
       " 'and',\n",
       " 'can',\n",
       " 'not',\n",
       " 'stand',\n",
       " '-PRON-',\n",
       " 'smell',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.lemma_ for w in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Another common step consists in filtering out very common words, called stop words. Words such as \"I\", \"the\", \"or\" and so on appear in so many phrases that they do not carry much information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afraid', 'bear', 'stand', 'smell', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.lemma_ for w in doc if not w.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word of warning: All these steps destroy information. If \"am\" and \"was\" collapse to \"be\", and if \"they\", \"she\" and \"I\" all become \"-PRON-\" then information is lost. Depending on the application this may be acceptable since, at the same time, we also gain something, namely a substantial reduction in vocabulary. In this sense, these preprocessing steps work as a dimension reduction method.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
