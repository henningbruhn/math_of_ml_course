{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing in natural language processing\n",
    "\n",
    "A common NLP library in python is spacy. You might need to install it first via \"pip install spacy\", or if you're running this in google colab, via \"!pip install spacy\". First we import spacy and load the corpus, ie, the text database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "First step: split the text sample into \"tokens\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'bears',\n",
       " 'and',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'stand',\n",
       " 'their',\n",
       " 'smell',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"I'm afraid of bears and can't stand their smell.\")\n",
    "[w.text for w in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might seem weird: \"'m\" and \"n't\" have each become a token. That is because they do represent a part of the speech that is really separate from the word they're in. We see in the next step why treating them separately is a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "Lemmatisation transforms a token into its base form. \"am\" or \"'m\" becomes \"be\", \"bears\" becomes \"bear\" and so on. \"I/you/she\" etc becomes \"-PRON-\", a marker for \"pronoun\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'bear',\n",
       " 'and',\n",
       " 'can',\n",
       " 'not',\n",
       " 'stand',\n",
       " '-PRON-',\n",
       " 'smell',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.lemma_ for w in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Another common step consists in filtering out very common words, called stop words. Words such as \"I\", \"the\", \"or\" and so on appear in so many phrases that they do not carry much information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afraid', 'bear', 'stand', 'smell', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.lemma_ for w in doc if not w.is_stop]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
