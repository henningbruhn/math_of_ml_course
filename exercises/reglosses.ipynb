{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac295d1a-503e-49f9-b900-61d2c9dc35d1",
   "metadata": {},
   "source": [
    "# Regression losses\n",
    "\n",
    "In this notebook you'll explore three regression loss functions: *mse*, *mae* and *mape*.\n",
    "\n",
    "First some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45457b-feb3-49fd-87f4-dd73f5507d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    plt.style.use(\"seaborn-v0_8\")\n",
    "except:\n",
    "    plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48f519-dfaa-40c1-aa8f-2d78625801aa",
   "metadata": {},
   "source": [
    "## Convenience code\n",
    "\n",
    "This section contains data generation and plotting code. **Don't look at it -- simply skip to the next section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c266-b4e6-4259-be14-ab7cd2463994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_data(N,seed=42):\n",
    "    np.random.seed(seed)\n",
    "    x=np.random.normal(size=N)\n",
    "    y=2.3*x+0.2+np.random.normal(size=N)\n",
    "    return x,y\n",
    "\n",
    "def add_outlier(x,y):\n",
    "    N=len(y)\n",
    "    xx=np.empty(N+1)\n",
    "    xx[:N]=x.copy()\n",
    "    xx[N]=1.1\n",
    "    yy=np.empty(N+1)\n",
    "    yy[:N]=y.copy()\n",
    "    yy[N]=8\n",
    "    return xx,yy\n",
    "\n",
    "def rand_data_outlier(N,seed=42):\n",
    "    return add_outlier(*rand_data(N,seed=seed))\n",
    "\n",
    "def rand_data2(N,seed=42):\n",
    "    np.random.seed(seed)\n",
    "    N=N//2\n",
    "    x1=np.random.random(size=N)*2+1\n",
    "    x2=np.random.random(size=N)*2+11\n",
    "    y1=2.3*x1+0.2+np.random.random(size=N)-0.5\n",
    "    y2=2.3*x2+0.2+np.random.random(size=N)*20-5\n",
    "    \n",
    "    return np.hstack([x1,x2]),np.hstack([y1,y2])\n",
    "\n",
    "def rand_data3(N,seed=42,small=10):\n",
    "    np.random.seed(seed)\n",
    "    x=np.empty(N)\n",
    "    y=np.empty(N)\n",
    "    x[:-small]=np.random.random(size=N-small)*10\n",
    "    y[:-small]=np.abs(2.3*x[:-small]+0.5+2*np.random.random(size=N-small)-1)\n",
    "    x[-small:]=np.random.random(size=small)*0.01\n",
    "    y[-small:]=np.random.random(size=small)*0.01\n",
    "    return x,y\n",
    "\n",
    "def plot_data(x,y,ax=None):\n",
    "    if ax is None:\n",
    "        fig,ax=plt.subplots(figsize=(6,4))\n",
    "    ax.scatter(x,y,color='b',edgecolor='k',alpha=0.5,s=10,zorder=10)\n",
    "    return ax   \n",
    "\n",
    "def plot_fits(ax,*args):\n",
    "    xx=np.array(ax.get_xlim())\n",
    "    for arg in args:\n",
    "        a,b,label=arg\n",
    "        ax.plot(xx,a*xx+b,linewidth=2,label=label,alpha=0.8)\n",
    "    ax.legend()\n",
    "\n",
    "def plot_errors(*args,cols=4):\n",
    "    n=len(args)\n",
    "    rows=math.ceil(n/cols)\n",
    "    if n<cols:\n",
    "        cols=n\n",
    "    fig,axs=plt.subplots(rows,cols,figsize=(cols*3,rows*3))\n",
    "    for arg,ax in zip(args,axs.flat):\n",
    "        errors,label=arg\n",
    "        ax.hist(errors,edgecolor='k')\n",
    "        ax.set_title(\"histogram \"+label+\" errors\")\n",
    "        ax.set_ylabel(\"count\")\n",
    "        ax.set_xlabel(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b819155-b104-4420-ba12-b15dfdb03fd8",
   "metadata": {},
   "source": [
    "## A first fit\n",
    "\n",
    "The function <code>rand_data</code> returns some sample data. We can easily plot it with the function <code>plot_data</code> that I've provided above. The function returns <code>ax</code>, a handle to the current plot. We'll need to pass that around later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab1d72-0a68-48ae-92eb-241c51009e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=rand_data(20)\n",
    "ax=plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66304a2a-3ed0-49d8-a163-089cc12dd9d2",
   "metadata": {},
   "source": [
    "So, let's do a linear regression with mse loss function. Fortunately, *scikit-learn* implements a corresponding class that we can use. For convenience, I've wrapped that into function that returns the two fit parameters <code>a,b</code> that describe the fitted line $ax+b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba7898-0685-4490-87d9-2d1d5129dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_fit_coefs(x,y):\n",
    "    msereg=LinearRegression()\n",
    "    msereg.fit(x.reshape(-1,1),y) # for technical reasons x needs to have two dimensions\n",
    "    a,b=*msereg.coef_,msereg.intercept_  # a,b as in ax+b\n",
    "    return a,b\n",
    "\n",
    "a,b=mse_fit_coefs(x,y)\n",
    "print(\"a={:.2f}, b={:.2f}\".format(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff0960-f9f9-46e3-b525-89588e3fe475",
   "metadata": {},
   "source": [
    "Why the weird <code>\\*</code> in front of <code>msereg.coef_</code>? That's the *unpacking* operator. The function <code>.coef_</code> returns a *list* of the fit parameters. In our case, however, the list will just contain a single parameter, namely $a$ of $ax+b$, because the intercept $b$ is stored separately. In order to store the single number $a$ in <code>a</code> and **not** the list <code>[a]</code> we use the unpack operator <code>\\*</code>. \n",
    "\n",
    "Next, we plot data and fitted line. Here, I am providing a convenience function as well (implementation above): <code>plot_fits</code>. It needs as first argument <code>ax</code>, the current plot, and then any number of tuples of the form <code>(a,b,'mse')</code>, s.t. <code>a,b</code> are the parameters of the fitted line and <code>label</code> simply is a label that appears in the legend of the plot. Let me demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe9b8f-bc94-4a43-90e4-0888b4f6020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=rand_data(20) # generate data\n",
    "a,b=mse_fit_coefs(x,y) # do the fit \n",
    "\n",
    "ax=plot_data(x,y) # plot the data\n",
    "plot_fits(ax,(a,b,'mse')) # plot the fitted line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006cc569-ffbc-452b-88e8-833c2abce733",
   "metadata": {},
   "source": [
    "Good. Next, we do a MAE fit. Unfortunately, linear regression with MAE loss does not seem to be directly supported by *scikit-learn*. We'll use a somewhat ad-hoc implementation that seems to work well. I don't think this implementation should be used in a serious context, though.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1dfe-4087-45e6-af66-d3989a55fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def L1_loss(params,x,y):\n",
    "    a,b=params\n",
    "    return np.mean(np.abs(y-a*x-b))\n",
    "\n",
    "def mae_fit_coefs(x,y):\n",
    "    start=(0,0)\n",
    "    result=minimize(L1_loss,start,args=(x,y))\n",
    "    a,b=result.x\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20826e-4f45-420b-9c0a-1da48e79aa2f",
   "metadata": {},
   "source": [
    "Next, we do both fits, and plot them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40f997-b33a-40a9-93f5-558c76858028",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=rand_data(20)\n",
    "\n",
    "a2,b2=mse_fit_coefs(x,y)\n",
    "a1,b1=mae_fit_coefs(x,y)\n",
    "\n",
    "ax=plot_data(x,y)\n",
    "plot_fits(ax,(a2,b2,'mse'),(a1,b1,'mae'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d19dba-250d-4c24-a62d-98fb40113bf2",
   "metadata": {},
   "source": [
    "We can also look at a histogram of the errors $(y-y^*)^2$ and $|y-y^*|$. We first need to compute them, and then we use a third, and last, convenience function <code>plot_errors</code>. As arguments <code>plot_errors</code> accepts *any* number of tuples <code>(errors,label)</code>, where <code>errors</code> contains a list of the errors and <code>label</code> is simply a string that describes the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c70a3-4a55-44f1-aca3-b597065f0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors1=np.abs(y-a1*x-b1)\n",
    "errors2=(y-a2*x-b2)**2\n",
    "plot_errors((errors2,'mse'),(errors1,'mae'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37504e6-3543-495f-8021-5bfa3b9574ba",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Let's see how outliers influence the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb8c35-592a-4030-9bee-3d303a7ce825",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_outlier,y_outlier=rand_data_outlier(20)\n",
    "\n",
    "a2,b2=mse_fit_coefs(x_outlier,y_outlier)\n",
    "a_clean,b_clean=mse_fit_coefs(x_outlier[:-1],y_outlier[:-1])\n",
    "\n",
    "ax=plot_data(x_outlier,y_outlier)\n",
    "ax.scatter([x_outlier[-1]],[y_outlier[-1]],color='r')\n",
    "plot_fits(ax,(a2,b2,'mse'),(a_clean,b_clean,'without outlier'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90f37d-16d1-416a-83b8-4d28dca39c6f",
   "metadata": {},
   "source": [
    "We see that the fit line moves towards the outlier (marked in red). Now it's your turn.\n",
    "\n",
    "### Task: outlier\n",
    "Fit a linear regression with *mse* loss, with *mae* loss and with *Huber* loss to the outlier data. A Huber regressor is provided by *scikit-learn*, see code below. Then plot the data as well as fitted lines. For the latter, use <code>plot_fits</code> (simply pass on three tuples as arguments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b535de-4e7b-4c6e-8431-0a8c92960bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Huber regressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "def huber_fit_coefs(x,y):\n",
    "    huber=HuberRegressor()\n",
    "    huber.fit(x.reshape(-1,1),y)\n",
    "    a,b=*huber.coef_,huber.intercept_    \n",
    "    return a,b\n",
    "\n",
    "### insert your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19437032-39c0-45df-ac80-f2c258de016f",
   "metadata": {},
   "source": [
    "### Task: outlier errors\n",
    "For *mse* and for *mae* use <code>plot_errors</code> to plot the histogram of errors. Answer in one sentence: What can you observe that explains why the *mse* loss moves the fit line stronger towards the outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d60bce-dc6c-4f2b-afb3-3f91e81f1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3a7ae-c709-4793-a638-6cc314fbb6ae",
   "metadata": {},
   "source": [
    "## MAPE\n",
    "\n",
    "I've also implemented linear regression with *mape* loss. Again, this is probably not an implementation intended for use in a production system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec65f2b-0d79-4cd0-b49e-b25f6185e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss(params,x,y):\n",
    "    a,b=params\n",
    "    return np.mean(np.abs(y-a*x-b)/y)\n",
    "\n",
    "def mape_fit_coefs(x,y):\n",
    "    start=(1,1)\n",
    "    result=minimize(mape_loss,start,args=(x, y),method=\"Nelder-Mead\")\n",
    "    a,b=result.x\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3136665-81c6-4ab7-9d2a-377ab14144a2",
   "metadata": {},
   "source": [
    "### Task: mape\n",
    "Below do a fit with *mse* loss, and a fit with *mape* loss. For *mse*, store $a,b$ of the fitted line $ax+b$ in <code>a2,b2</code>, and for *mape* put them in <code>a_mape,b_mape</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb6af4-9d64-4029-8337-5183be25a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=rand_data2(200)\n",
    "\n",
    "### insert your code here ###\n",
    "\n",
    "### end of insert ###\n",
    "\n",
    "x,y=rand_data2(200)\n",
    "fig,axs=plt.subplots(1,2,figsize=(12,4))\n",
    "ax1,ax2=axs\n",
    "\n",
    "plot_data(x,y,ax=ax1)\n",
    "plot_fits(ax1,(a2,b2,'mse'),(a_mape,b_mape,'mape'))\n",
    "\n",
    "ax2.set_xlim(0,4)\n",
    "ax2.set_ylim(-0.5,10)\n",
    "plot_data(x,y,ax=ax2)\n",
    "plot_fits(ax2,(a2,b2,'mse'),(a_mape,b_mape,'mape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941236d6-354e-4341-8cef-ed2378dee2ec",
   "metadata": {},
   "source": [
    "### Task: mape errors\n",
    "Using <code>plot_errors</code> plot a histogram of the *mse* and of the *mape* errors. Answer in at most three sentences: Which loss is more suitable here, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab3680-05aa-4d32-8e18-3c09785b133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0aec4-42c3-40c5-91bc-1d1692ef3ed1",
   "metadata": {},
   "source": [
    "### Task: More mape\n",
    "* Do an *mse* fit, and store the fit parameters in <code>a2,b2</code>.\n",
    "* Do a *mape* fit, and store the fit parameters in <code>a_mape,b_mape</code>.\n",
    "* Plot the data and both fits.\n",
    "* Plot the error histogram for both fits.\n",
    "* Answer in at most three sentences: Which fit is better suited and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2c6eb-28ad-40bf-a7fb-a32be404ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=rand_data3(100,small=5)\n",
    "\n",
    "### insert your code here ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6a33d-25a8-4e3e-b066-e8bb4e0249ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
