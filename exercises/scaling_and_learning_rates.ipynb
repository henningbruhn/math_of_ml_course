{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013023df-750d-4525-9f98-d82bcd7ac2c8",
   "metadata": {},
   "source": [
    "# Exercise: Scaling and learning rates\n",
    "\n",
    "In this notebook, we'll train a neural network on a EEG dataset. The objective is to detect whether the eyes of the experimental subject are open or closed.  \n",
    "\n",
    "First we do a couple of necessary imports and load the dataset. We also print the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856dab2-c8ed-4dd2-b4f3-cb56819761c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# fetch data from openml.org\n",
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml('eeg-eye-state', cache=True)\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99777749-8ac9-49d5-8f69-5ede3dc453c7",
   "metadata": {},
   "source": [
    "Let's start examining the dataset. First question: how large is the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06199cb6-e0f5-4861-8e0f-fa546d8e51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples: {}\".format(len(data[\"data\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0619c03-cb5c-4c20-8a65-fd547379baf1",
   "metadata": {},
   "source": [
    "Next, we split off a training set and a validation set. Because there is not much data and we just want to compare different algorithms, we don't define a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f0cae-3a17-4b09-8660-f127133a559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=sklearn.utils.shuffle(np.array(data[\"data\"]), data[\"target\"].astype('int')-1) # let's make sure the data is in random order\n",
    "train_size=10000\n",
    "X_train,X_val=X[:train_size],X[train_size:]\n",
    "y_train,y_val=y[:train_size],y[train_size:]\n",
    "X_train.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532eced-5256-4a3f-992d-a0ee68ff17ff",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We continue inspecting the data. As I already know that the data takes continuous values with the values mainly in the same range, we can do a boxplot of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68027ca6-7cb9-4c1f-8154-a2bfb0cb590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs=plt.subplots(1,2,figsize=(10,5))\n",
    "axs[0].boxplot(X_train[:,:5])\n",
    "axs[0].set_title(\"feature range of first five features, with outliers\")\n",
    "axs[1].boxplot(X_train,showfliers=False)\n",
    "axs[1].set_title(\"feature range, without outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b7e23-11e6-4165-b9cd-a6b9e5440de7",
   "metadata": {},
   "source": [
    "We observe: there are substantial outliers and the data takes quite large values.\n",
    "\n",
    "### Task: Train NN\n",
    "* Use tensorflow, to define a ReLU-neural network for binary classification with two hidden layers, the first with 50 neurons, the second with 10 neurons. You can either have a single output neuron with logistic activation ('sigmoid') or two output neurons with softmax activation.\n",
    "* Take <code>tf.keras.losses.SparseCategoricalCrossentropy</code> as loss.\n",
    "* Train the neural network for 30 epochs and pass along the validation set <code>validation_data=(X_val,y_val)</code> to <code>fit</code>. Otherwise use the default parameters.\n",
    "\n",
    "You should see terrible performance. (If this exercise seems challenging to you, have a look in [tfintro](https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/tfintro.ipynb).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f40b9-bded-4afc-8e89-c1b82688138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0b4e3-2e9f-4fba-9bba-711933903a84",
   "metadata": {},
   "source": [
    "What happened? Obviously, the model performs not well, not at all. The reason: When the weights are initialised at the beginning of training it is expected that the data has values more or less in the range $[-1,1]$. Our data, however, takes much larger values. Because of that the neural network starts training in a sort of off-balance state and then takes a very long while moving away from there (if that happens at all). We'll rectify that by scaling the data. Here, because it's simple we use <code>sklearn.preprocessing.StandardScaler</code>, a method of *scikit-learn*. There's also a tensorflow way of doing this, which you'll see in the solution. \n",
    "\n",
    "What does <code>StandardScaler</code> do? First, it ensures that the data has mean 0 by substracting the mean from every sample. Then the data is multiplied by a factor so that the variance becomes 1. Thus, larger values than 1 are possible, but mostly the values range from -1 to 1. Let's have a look at the boxplots again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c6b5b-9b33-46d9-a963-7bec8f39ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # we need to fit, to learn the mean and the variance\n",
    "X_val_scaled=scaler.transform(X_val) # the validation/test set is only transformed, not fit \n",
    "\n",
    "### plotting\n",
    "_,axs=plt.subplots(1,2,figsize=(10,5))\n",
    "axs[0].boxplot(X_train_scaled[:,:5])\n",
    "axs[0].set_title(\"feature range of first five features, with outliers\")\n",
    "axs[1].boxplot(X_train_scaled,showfliers=False)\n",
    "axs[1].set_title(\"feature range, without outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41d080-b5e5-4810-b1be-bf254a91fc4e",
   "metadata": {},
   "source": [
    "Note that the line in the boxes shows the *median* not the *mean* (which should be at 0). There are still substantial outliers but as most of the values hail from $[-1,1]$, training should hopefully work better now. Let's try out!\n",
    "\n",
    "### Task: Train with scaling\n",
    "* Repeat the steps of the previous task, use <code>X_train_scaled</code> and <code>X_val_scaled</code>, however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ca952-9ee0-4d5d-946c-3c8cf1aa5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81e2b2-da6b-4a8e-9e88-8bf804d7921b",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a4749-1097-44d8-8b42-da422240649b",
   "metadata": {},
   "source": [
    "## Learning rates\n",
    "\n",
    "Next, we'll see how learning rates can be set. So far, we had always used the default values for the optimiser. This needs to change now. The optimiser is specified when calling <code>compile</code> on a model. If we just want to choose between different optimisers without setting any optimiser specific parameters, we can prescribe the optimiser by passing a string: \n",
    "\n",
    "<code>mode.compile(loss=loss_fn, metrics=['accuracy'],optimizer=\"SGD\")  # stochastic gradient descent</code> \n",
    "\n",
    "or \n",
    "\n",
    "<code>mode.compile(loss=loss_fn, metrics=['accuracy'],optimizer=\"Adam\")  # Adam optimiser</code> \n",
    "\n",
    "As we want set a learning rate, however, we actually need to instantiate an optimiser object:\n",
    "\n",
    "<code>my_SGD=tf.keras.optimizers.SGD(learning_rate=0.42)\n",
    "mode.compile(loss=loss_fn, metrics=['accuracy'],optimizer=my_SGD)</code> \n",
    "\n",
    "### Task: different learning rates\n",
    "* For each of the learning rates $[0.01,0.1,1]$ train a neural network of the same type as above (ie, two hidden layers with 50 and 10 neurons etc), again for 30 epochs each.\n",
    "* Use the <code>history</code> object returned by <code>fit</code> to plot the three losses (for the different learning rates) in the same plot. Which learning rate works best? For plotting see [plt_intro](https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/python_intro/plt_intro.ipynb).\n",
    "* Plot also the validation loss. (You find it in <code>history.history[\"val_loss\"]</code>.)\n",
    "\n",
    "By the way, if all the output of <code>fit</code> annoys you: You can switch it off by setting <code>verbose=0</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463180fc-c89d-4b97-a2b0-832fc9651d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
