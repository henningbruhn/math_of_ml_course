{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f5a899-76d0-4b4f-a9d0-307e19162bad",
   "metadata": {},
   "source": [
    "**<div align=\"center\"><span style=\"font-size:4em\">Exercise</span></div>**\n",
    "\n",
    "# Mushrooms\n",
    "\n",
    "In this notebook you're expected to practice setting up and training a simple neural network with tensorflow. If you're running this in Colab then you will not need to install any packages. If you run this on your own compute, you may need to install tensorflow, scikit-learn, numpy or matplotlib.\n",
    "\n",
    "First we do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57e18b-c6c3-4842-b8fc-7ecfefd53be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "print(\"Tensorflow version: \"+tf.version.VERSION)\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd04e2-a9d5-421d-9ff1-a4ecfec9e0b9",
   "metadata": {},
   "source": [
    "Next we download a simple dataset on mushrooms. The task will consist in predicting whether a given mushroom is edible or poisonous. The dataset itself is hosted on a repository for a number of datasets used in machine learning: [openml](https://www.openml.org/) To download the data, we use a convenient method of scikit-learn (and that is also the reason why we're import scikit-learn at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fec14c-f9bf-426f-90e0-3865bec0b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target=sklearn.datasets.fetch_openml('mushroom',return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf44b4-8645-43cb-9134-6d8fc7923316",
   "metadata": {},
   "source": [
    "Okay, a warning that in the future something will change. That shouldn't bother us. Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8389de-106c-4beb-bebe-a7f8bfa665c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bea814-9a7c-4cdf-a964-5fd077de4d36",
   "metadata": {},
   "source": [
    "Okay, apparently each mushroom is described by a number of characteristics that will be familiar to every mushroom lover. What matters for us: the features are not encoded numerically -- we'll need to change that. But first let's look at the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0331277-ea02-421e-ba51-57c83ebc895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5f5e1-664d-43e2-9975-42d18e26c546",
   "metadata": {},
   "source": [
    "Aha, two values 'e' (as in *edible*) and 'p' (as in *rather not*). Also here: we'll need to change that to a numerical value. Let's start with the target. We simply put a '1' whenever the mushroom in question is poisonous. The code looks a bit more complicated because I want to the target to be encoded as a *float* vector, ie, with values 1.0 and 0.0. (If we don't specify that we'll get a vector of <code>True</code> and <code>False</code> values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231594a-d58b-4e40-9dfb-303cc9c0ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=np.array(target=='p',dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf886c7-3a2c-47fe-9883-6616d060ba55",
   "metadata": {},
   "source": [
    "Next, let's transform the data. *scikit-learn* provides a convenient class to do that, the <code>OneHotEncoder</code>. *tensorflow* offers similar functionality but it's a little bit more complicated to use so we'll go with *scikit-learn*. Let's look at a small toy example to figure out how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebfe3f-980e-4347-b087-a528bf528e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=[['cat','light saber'],['dog','stick'],['rat','light saber']]\n",
    "\n",
    "one_hot_encoder=sklearn.preprocessing.OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoder.fit_transform(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bdbe9-5c37-44ae-8db4-9178a9764391",
   "metadata": {},
   "source": [
    "The first three entries in each row encode *cat*, *dog*, *rat*, the last two encode *light saber* and *stick*.\n",
    "\n",
    "Let's now apply that to the mushroom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc37b8-5de7-48f6-8dac-ee664cdd8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder=sklearn.preprocessing.OneHotEncoder(sparse_output=False)\n",
    "XX=one_hot_encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d53ed-7e70-491f-a36b-60cb4cdc64ac",
   "metadata": {},
   "source": [
    "Let's check how the data has changed. Before we had (compare the table above) 8124 datapoints with 22 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58c4f9-e2f3-4d65-a069-ce947902e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c18f9-3321-4e5d-9d41-3c1f83cec875",
   "metadata": {},
   "source": [
    "We split the data into three sets: A training set, a test set and a validation set that we will use to estimate the final error during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b58194-b0ad-4268-bc41-f6c00c953a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=sklearn.utils.shuffle(XX,yy) # let's make sure the data is in random order\n",
    "train_size=6000\n",
    "val_size=1000\n",
    "X_train,X_val,X_test=X[:train_size],X[train_size:train_size+val_size],X[train_size+val_size:]\n",
    "y_train,y_val,y_test=y[:train_size],y[train_size:train_size+val_size],y[train_size+val_size:]\n",
    "X_train.shape,X_val.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e546e-dedf-4c1d-b208-9be584bc52af",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "\n",
    "Set up a neural network, train it and plot training loss and loss on the validation set. A good idea would be to consult the notebook [tfintro](https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/neural_networks/tfintro.ipynb) for some pointers. For help on plotting, also look at [plt_intro](https://colab.research.google.com/github/henningbruhn/math_of_ml_course/blob/main/python_intro/plt_intro.ipynb).\n",
    "\n",
    "More concretely\n",
    "* Define a neural network with a single output neuron. Experiment with the size of the network. That is, see how many neurons and layers you actually need. Since we have only one output, the activation in the last layer should be <code>activation='sigmoid'</code>, which is nothing else than the logistic function.\n",
    "* Take <code>BinaryCrossentropy</code> as loss function. That is cross entropy adapted to binary classification, ie, when there is a single output and classes are encoded as 0 and 1.\n",
    "* Train the network for suitable number of rounds (<code>epochs</code>). When calling <code>fit</code> use also the parameter <code>validation_data=(X_val,y_val)</code> to record loss and accuracy on the validation set.\n",
    "* Collect the result of training in a variable <code>history</code> and plot training loss and validation loss in one plot, and training accuracy and validation accuracy in another (side by side, preferably).\n",
    "* Compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb2ae4-6284-4f20-ae29-a38ec9aa2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### put your code here and below ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
