{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2PEXhoF_8bY"
   },
   "source": [
    "# Exercise: Polynomial regression\n",
    "\n",
    "In this exercise we train polynomial regressors. Given data $(x_1,y_1),\\ldots, (x_n,y_n)$ we search for polynomial regressors $f$ such that $$\\sum_{i=1} (y_i-f(x_i))^2$$ becomes small.\n",
    "\n",
    "First we do some of the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvR52WJy_8bc"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # for general scientific computing\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import sklearn.linear_model # linear regression \n",
    "import sklearn.preprocessing # for polynomial regression (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbXQ3HRCRBMP"
   },
   "source": [
    "We generate a small training and test set. Both sets come from the quadratic function $1.5x^2-2x+1$, with a little bit of random noise added. To illustrate both sets we plot them, as well as the quadratic function. \n",
    "\n",
    "You do not need to understand the code below in full detail. There is just one detail I'd like to point out. The estimators of *scikit learn* expect the training set to have a particular format: a list (or rather an array) of (multidimensional) data vectors. Because of that the estimators will raise an error if the training set is 1-dimensional. Consequently, we need to <code>reshape</code> the data. That is, we turn 1-dimensional data into 2-dimensional data. Let me demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim=np.array([1.,2.,42.])\n",
    "one_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the training set consisting of data <code>x_train</code> and target values <code>y_train</code>, and the test set consisting of <code>x_test</code> and <code>y_test</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Tnb3gFF0_8bw",
    "outputId": "e9cfe912-582a-472c-b1e7-df89f39ab390"
   },
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "    return 2.5*x**2-2*x+1\n",
    "\n",
    "def draw_points(number):\n",
    "    np.random.seed(42*number)\n",
    "    x = np.random.random(number)\n",
    "    noise = np.random.normal(scale=0.05,size=number)\n",
    "    y = true_function(x) + noise\n",
    "    return x.reshape(-1,1),y # x reshaped because training set cannot be 1-dim\n",
    "\n",
    "training_size = 10\n",
    "x_train,y_train=draw_points(training_size)\n",
    "\n",
    "test_size = 5\n",
    "x_test,y_test = draw_points(test_size)\n",
    "\n",
    "xx=np.linspace(0,1,200)\n",
    "yy_true=true_function(xx)\n",
    "fig,ax=plt.subplots(figsize=(5,5))\n",
    "ax.plot(xx,yy_true,\"b-\",label=\"true curve\",linewidth=4)\n",
    "ax.plot(x_test.flat,y_test,\"rx\",label=\"test set\")\n",
    "ax.plot(x_train.flat,y_train,\"go\",label=\"training set\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a linear regression. For this, we first instantiate a <code>LinearRegression</code> object, which we find in the package <code>sklearn.linear_model</code>. Then it needs to be trained. All classifiers and regressors in *scikit learn* have a method <code>fit</code> for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg=sklearn.linear_model.LinearRegression()\n",
    "lin_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, we can <code>predict</code> new values. The method <code>predict</code> expects the $x$-values in the same format as the training set, ie, a list of(multidimensional) data points. That means, even, if you just need the prediction for a single data point, you need to wrap it in square brackets: <code>estimator.predict([single_x])</code>. \n",
    "\n",
    "Because we have 1-dimensional $x$ we even need to put the data into two square brackets. Let's predict two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.predict([[1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is not very interesting. Let's do quadratic regression. We already know that quadratic regression is just linear regression with a modified training set. In *scikit learn* we can easily compute features of any degree with <code>PolynomialFeatures</code>. In the same way that estimators need to be fit first, this is also the case for <code>PolynomialFeatures</code>. <code>fit</code> does not do much here: It simply memorises the dimension of the data. Instead of a <code>predict</code> method, we have here a <code>transform</code> method. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_features=sklearn.preprocessing.PolynomialFeatures(degree=2)\n",
    "x_demo=np.array([0,1,2,3]).reshape(-1,1) # reshape because estimator expects multidim training set\n",
    "quad_features.fit(x_demo)\n",
    "quad_features.transform(x_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data <code>[2]</code> turns into <code>[1,2,4]</code>, ie, constant term, linear term and quadratic term, as expected.\n",
    "\n",
    "Next, we can plug the new, transformed training set into a linear regression, and obtain a quadratic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=sklearn.linear_model.LinearRegression()\n",
    "reg.fit(quad_features.transform(x_train),y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a bit annoying: If we now want to do predictions, we first need to <code>transform</code> the $x$-values via <code>quad_features</code> and then call <code>predict</code> on <code>reg</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(quad_features.transform([[1],[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because that is so annyoing, *scikit learn* has the class <code>Pipeline</code> that allows to chain any number of transformators and a <code>predictor</code>. Once the pipeline is defined, it can be used like a normal regressor, ie, we can call <code>fit</code> and also <code>predict</code> on it. Here's how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline\n",
    "quad_features=sklearn.preprocessing.PolynomialFeatures(degree=2)\n",
    "reg=sklearn.linear_model.LinearRegression()\n",
    "### this what we want to chain:\n",
    "steps=[('quadratic features',quad_features),('linear regression',reg)] # always pairs (name, estimator)\n",
    "quad_pipe=sklearn.pipeline.Pipeline(steps)\n",
    "quad_pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole pipeline is now trained and can be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_pipe.predict([[1],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Train degree 10 regressor###\n",
    "\n",
    "In the same way as above, define a regressor of degree 10 with a pipeline and train it on the training set. The <code>Pipeline</code> object that encapsulates the regressor should be called <code>ten_pipe</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###\n",
    "### end of insert ###\n",
    "\n",
    "ten_pipe.predict([[1],[3]]) # this should run without any errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Okr-YQQSBIA"
   },
   "source": [
    "Let's plot the result. (This will not work if you messed up the regressors or if you didn't call the )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,5))\n",
    "ax.set_ylim(0,2)\n",
    "xx=np.linspace(0,1,400)\n",
    "estimators=[(1,lin_reg),(2,quad_pipe),(10,ten_pipe)]\n",
    "estimator_degrees=[1,2,10]\n",
    "for degree,estimator in estimators:\n",
    "    ax.plot(xx,estimator.predict(xx.reshape(-1,1)),linewidth=3,label=\"degree {}\".format(degree))\n",
    "ax.plot(x_test.flat,y_test,\"rx\",label=\"test set\")\n",
    "ax.plot(x_train.flat,y_train,\"go\",label=\"training set\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute errors. For regression, we often use the *mean square error*, or mse for friends:\n",
    "$$\n",
    "\\text{mse}(y,y')=\\frac{1}{n}\\sum_{i=1}^n(y_i-y'_i)^2\n",
    "$$\n",
    "\n",
    "The mean square error is implemented as function <code>mean_squared_error</code> in package <code>sklearn.metrics</code>. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "mse=sklearn.metrics.mean_squared_error\n",
    "y1=[1,1,1]\n",
    "y2=[1.1,0.9,1.1]\n",
    "mse(y1,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSXA6Ut4TCjd"
   },
   "source": [
    "### Task: errors ###\n",
    "\n",
    "Compute the training error and test error for the three regressor we've trained above. Output the errors with <code>print</code>. To print out text and variable at the same time you can do the following <code>print(\"this {} is the value of the variable\".format(some_variable))</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: interpretation ###\n",
    "\n",
    "Interpret the results, give an explanation for the training and test errors (at most three sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercise02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
